{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5026615a",
   "metadata": {},
   "source": [
    "# Computational Social Science Project #2 \n",
    "\n",
    "**Enter your Name:** Usama Faheem \n",
    "\n",
    "*Semester:* Fall 2023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceabce6",
   "metadata": {},
   "source": [
    "Below we fill in some of the code you might use to answer some of the questions. Here are some additional resources for when you get stuck:\n",
    "* Code and documentation provided in the course notebooks  \n",
    "* [Markdown cheatsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) to help with formatting the Jupyter notebook\n",
    "* Try Googling any errors you get and consult Stack Overflow, etc. Someone has probably had your question before!\n",
    "* Send me a pull request on GitHub flagging the syntax that's tripping you up "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f76ede",
   "metadata": {},
   "source": [
    "**INSTRUCTIONS:** For this project, copy all of the files in the Project 2 folder in the course repo into a \"Project 2\" subfolder within the \"Computational Social Science Projects\" directory that you created for the first project. You will work on the project locally, push your project to GitHub, and submit a link to the GitHub repo on bCourses by the project deadline. Be sure the final submission is in the main branch, which is what I'll pull down and re-run to grade. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7920223d",
   "metadata": {},
   "source": [
    "## 1. Introduction/Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013cd72b",
   "metadata": {},
   "source": [
    "#### a) Import relevant libraries\n",
    "Here are some libraries you will need to get started. Along the way you may need to add more. Best practice is to add them here at the top of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca2b1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# import libraries you might need here \n",
    "#-----------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# use random seed for consistent results \n",
    "np.random.seed(273)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b23b0",
   "metadata": {},
   "source": [
    "#### b) Read in and inspect data frame \n",
    "Read in the data frame and look at some of its attributes. Read in the data contained in the projoect folder: \"Diabetes with Population Info by County 2017.csv\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171cd9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# read in and inspect data frame\n",
    "#-----------\n",
    "# Note that \"CountyFips\" needs to be a string so the leading 0 isn't dropped (only if you want to make choropleth map) \n",
    "diabetes = pd.read_csv(______, \n",
    "                       dtype={\"CountyFIPS\": str}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca77b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# look at shape\n",
    "#-----------\n",
    "# look at the dimensions of the diabetes data frame\n",
    "print('shape: ', diabetes.______) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91328e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# set pandas parameters\n",
    "#-----------\n",
    "# tells pandas how many rows to display when printing so results don't get truncated\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# look at the data types for each column in diabetes df (likely be located under each row bc column names are long)\n",
    "print('data types:', diabetes.______)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e77f8b",
   "metadata": {},
   "source": [
    "Immediately, we see that some of the features that should be numeric (e.g., Diabetes_Number, Obesity_Number,  and Physical_Inactivity_Number) are not. We can check to see what the non-numeric values are in a column where we are expecting numeric information with a combination of `str.isnumeric()` and `unique()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72315139",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# identify non-numeric features\n",
    "#-----------\n",
    "# Return rows where the column \"Diabetes_Number\" is non-numeric and get the unique values of these rows\n",
    "# the \"~\" below in front of diabetes negates the str.isnumeric() so it only takes non-numeric values\n",
    "print(diabetes[~diabetes[______].str.isnumeric()][______].unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1e47c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Now do the same as above, but for \"Obesity_Number\"\n",
    "#-----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Now do the same as above, but for \"Physical_Inactivity_Number\" \n",
    "#-----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b705636f",
   "metadata": {},
   "source": [
    "These values (\"Suppresssed\" and \"No Data\") contained in the two respective columns are coercing these features to objects instead of them being  integers. Let's drop those rows in the next section, and also recode \"Physical_Inactivity_Number\" feature to be an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a442a",
   "metadata": {},
   "source": [
    "#### c. Recode variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31fe962",
   "metadata": {},
   "source": [
    "Convert 'Diabetes_Number', 'Obesity_Number', and 'Physical_Inactivity_Number' to integers below so we can use them in our analysis. Also fill in the object type we want to recode 'sex and age_total population_65 years and over_sex ratio (males per 100 females)' too (you'll have to scroll all the way over to the right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4acf34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Recode variables\n",
    "#-----------\n",
    "\n",
    "# Diabetes\n",
    "# ----------\n",
    "# keep only useful info about our target feature, i.e., where diabetes_number not = 'Suppressed'\n",
    "# note that the inside reference to the diabetes df identifies the column, and the outer calls specific rows according to a condition \n",
    "diabetes = diabetes[diabetes['Diabetes_Number']!=\"Suppressed\"] \n",
    "\n",
    "# use the astype method on Diabetes_Number to convert it to an integer...if you are not sure, what does the astype() documentation tell you are possible arguments? \n",
    "diabetes['Diabetes_Number'] = diabetes['Diabetes_Number'].astype(______) \n",
    "\n",
    "# Obesity\n",
    "# ----------\n",
    "\n",
    "\n",
    "# Physical Inactivity\n",
    "# ----------\n",
    "\n",
    "\n",
    "# Some final changes \n",
    "# ----------\n",
    "# 65+ sex ratio had one \"-\" in it so let's drop that row first\n",
    "diabetes = diabetes[diabetes['sex and age_total population_65 years and over_sex ratio (males per 100 females)']!= \"-\"]\n",
    "\n",
    "# change to numeric from string, since it originally included the \"-\", which made it a string\n",
    "# you'll have to decide whether to make it integer or float \n",
    "diabetes['sex and age_total population_65 years and over_sex ratio (males per 100 females)'] = diabetes['sex and age_total population_65 years and over_sex ratio (males per 100 females)'].astype(______)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a6850c",
   "metadata": {},
   "source": [
    "We should probably scale our count variables to be proportional to county population. We create the list 'rc_cols' to select all the features we want to rescale, and then use the `.div()` method to avoid typing out every single column we want to recode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb2299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Scale to county populations\n",
    "#-----------\n",
    "\n",
    "# select count variables to recode to percentages; make sure we leave out ratios and our population variable \n",
    "# because these don't make sense to scale by population\n",
    "rc_cols = [col for col in ______.columns if col not in ['County', 'State', 'CountyFIPS', \n",
    "                                                        'sex and age_total population_65 years and over_sex ratio (males per 100 females)', \n",
    "                                                        'sex and age_total population_sex ratio (males per 100 females)', \n",
    "                                                        'sex and age_total population_18 years and over_sex ratio (males per 100 females)',  \n",
    "                                                        'race_total population']]\n",
    "           \n",
    "# recode all selected columns to numeric\n",
    "diabetes[______] = diabetes[______].apply(pd.to_numeric, errors='coerce') \n",
    "\n",
    "# divide all columns but those listed above by total population to calculate rates\n",
    "diabetes[______] = diabetes[______].div(______['race_total population'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9e1fe2",
   "metadata": {},
   "source": [
    "Let's check our work. Are all rates bounded by 0 and 1 as expected? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f8f887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# check\n",
    "#-----------\n",
    "# set pandas options\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# inspect recoded values\n",
    "diabetes_summary = ______.describe().transpose() # note we use the transpose method rather than .T because this object is not a numpy array\n",
    "  \n",
    "# check recoding \n",
    "with pd.option_context('display.max_rows', 100, 'display.max_columns', None): \n",
    "    display(diabetes_summary.iloc[ : ,[0,1,3,7]]) # select which columns in the summary table we want to present"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dcc070",
   "metadata": {},
   "source": [
    "#### d. Check for duplicate columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acce0aa",
   "metadata": {},
   "source": [
    "There are a lot of columns in this data frame. Let's see if there are any are duplicates. Note that Pandas will not allow them to have the same exact column name, so they will likely be distinct on column name but will be copies otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700163cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Check for duplicate columns\n",
    "#-----------\n",
    "# I used Google to figure this out, and adapted this example for our purposes:  \n",
    "# source: https://thispointer.com/how-to-find-drop-duplicate-columns-in-a-dataframe-python-pandas/ \n",
    "def getDuplicateColumns(df):\n",
    "    '''\n",
    "    Get a list of duplicate columns.\n",
    "    It will iterate over all the columns in dataframe and find the columns whose contents are duplicate.\n",
    "    :param df: Dataframe object\n",
    "    :return: List of columns whose contents are duplicates.\n",
    "    '''\n",
    "    duplicateColumnNames = set()\n",
    "    # Iterate over all the columns in dataframe\n",
    "    for x in range(df.shape[1]):\n",
    "        # Select column at xth index.\n",
    "        col = df.iloc[:, x]\n",
    "        # Iterate over all the columns in DataFrame from (x+1)th index till end\n",
    "        for y in range(x + 1, df.shape[1]):\n",
    "            # Select column at yth index.\n",
    "            otherCol = df.iloc[:, y]\n",
    "            # Check if two columns at x 7 y index are equal\n",
    "            if col.equals(otherCol):\n",
    "                duplicateColumnNames.add(df.columns.values[y])\n",
    "    return list(duplicateColumnNames)\n",
    "\n",
    "duplicateColumnNames = list(getDuplicateColumns(______))\n",
    "print('Duplicate Columns are as follows: ')\n",
    "duplicateColumnNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba3a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# drop columns from duplicates list\n",
    "#-----------\n",
    "# now drop list of duplicate features from our df using the .drop() method\n",
    "diabetes = diabetes.drop(columns=______) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a85ec3",
   "metadata": {},
   "source": [
    "Finally, there are many states accounted for the in dataset. If we convert this column to a categorical variable, and create dummies, it will create a rather sparse matrix (many 0s in our dataset) becuase there will be 49 dummy variables. One alternative is to classify each state to a larger US region and use that variable instead of state. The following code will do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd769e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary mapping states to regions\n",
    "state_to_region = {\n",
    "    'Alabama': 'Southeast',\n",
    "    'Alaska': 'West',\n",
    "    'Arizona': 'West',\n",
    "    'Arkansas': 'South',\n",
    "    'California': 'West',\n",
    "    'Colorado': 'West',\n",
    "    'Connecticut': 'Northeast',\n",
    "    'Delaware': 'Northeast',\n",
    "    'District of Columbia': 'Southeast',\n",
    "    'Florida': 'Southeast',\n",
    "    'Georgia': 'Southeast',\n",
    "    'Hawaii': 'West',\n",
    "    'Idaho': 'West',\n",
    "    'Illinois': 'Midwest',\n",
    "    'Indiana': 'Midwest',\n",
    "    'Iowa': 'Midwest',\n",
    "    'Kansas': 'Midwest',\n",
    "    'Kentucky': 'South',\n",
    "    'Louisiana': 'South',\n",
    "    'Maine': 'Northeast',\n",
    "    'Maryland': 'Northeast',\n",
    "    'Massachusetts': 'Northeast',\n",
    "    'Michigan': 'Midwest',\n",
    "    'Minnesota': 'Midwest',\n",
    "    'Mississippi': 'South',\n",
    "    'Missouri': 'Midwest',\n",
    "    'Montana': 'West',\n",
    "    'Nebraska': 'Midwest',\n",
    "    'Nevada': 'West',\n",
    "    'New Hampshire': 'Northeast',\n",
    "    'New Jersey': 'Northeast',\n",
    "    'New Mexico': 'West',\n",
    "    'New York': 'Northeast',\n",
    "    'North Carolina': 'Southeast',\n",
    "    'North Dakota': 'Midwest',\n",
    "    'Ohio': 'Midwest',\n",
    "    'Oklahoma': 'South',\n",
    "    'Oregon': 'West',\n",
    "    'Pennsylvania': 'Northeast',\n",
    "    'Rhode Island': 'Northeast',\n",
    "    'South Carolina': 'Southeast',\n",
    "    'South Dakota': 'Midwest',\n",
    "    'Tennessee': 'South',\n",
    "    'Texas': 'South',\n",
    "    'Utah': 'West',\n",
    "    'Vermont': 'Northeast',\n",
    "    'Virginia': 'Southeast',\n",
    "    'Washington': 'West',\n",
    "    'West Virginia': 'South',\n",
    "    'Wisconsin': 'Midwest',\n",
    "    'Wyoming': 'West'\n",
    "}\n",
    "\n",
    "# Add a new 'Region' column based on the mapping\n",
    "diabetes['Region'] = diabetes['State'].map(state_to_region)\n",
    "\n",
    "# Print to verify'Region' column has been added\n",
    "diabetes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4007b8d1",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "Make at least two figures (feel free to make more) and explain their relevance to the scientific problem. The goal here is to uncover interesting patterns in the data, learn more about the scope of the problem, and communicate these findings to your audience in clear ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfb3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# EDA #1 and interpretations in this section \n",
    "#-----------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# EDA #2 and interpretations in this section \n",
    "#-----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df05b0",
   "metadata": {},
   "source": [
    "## 3. Prepare to Fit Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73492869",
   "metadata": {},
   "source": [
    "### 3.1 Finalize Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67257021",
   "metadata": {},
   "source": [
    "We've already cleaned up the data, but we can make a few more adjustments before partitioning the data and training models. Let's recode 'Region' to be a categorical variable using `pd.get_dummies` and drop 'State'. Also, we'll drop 'County' because 'CountyFIPS' is already a unique identifier for the county. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa1676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Drop and get dummies\n",
    "#-----------\n",
    "\n",
    "# create dummy features out of 'Region', which might be related to diabetes rates \n",
    "diabetes_clean = pd.get_dummies(______, \n",
    "                               columns = [______],  \n",
    "                               drop_first = True) # drop the first as a reference \n",
    "\n",
    "# drop 'County' and 'State' variables\n",
    "diabetes_clean = diabetes_clean.drop(labels = ['...', '...'],\n",
    "                               axis = ______) # which axis tells python we want to drop columns rather than index rows?\n",
    "\n",
    "# look at first 10 rows of new data frame \n",
    "diabetes_clean.______ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768ecd6",
   "metadata": {},
   "source": [
    "### 3.2 Partition Data, Feature Selection, and Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f305051f",
   "metadata": {},
   "source": [
    "Now, we will partition our data to prepare it for the training process. Ultimately we want to use a 60% train—20% validation—20% test in this case. More data in the training set lowers bias, but then increases variance in the validation/test sets. Balancing between bias and variance with choice of these set sizes is important as we want to ensure that there is enough data to train on to get good predictions, but also want to make sure our hold-out sets are representative enough.\n",
    "\n",
    "Work through partitioning the data into the test/train/validation sets in the chunks below. Be sure to that if you are using Ridge or LASSO, you Standardize the data. Where you do this in the workflow matters so be clear about where you are doing this and why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b28c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Partition data\n",
    "#-----------\n",
    "\n",
    "# import library\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create y dataframe \n",
    "y = ______\n",
    "\n",
    "# create X dataframe (include everything except \"Diabetes_Number\", our target, \n",
    "# and \"race alone or in combination with one or more other races_total population\")\n",
    "X = ______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56216906",
   "metadata": {},
   "source": [
    "Investigate whether there are any features that you should remove prior to spliting and model fitting. You may also consider using plots and relationships you found in the EDA stage for this question. Be sure to justify your logic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed567cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Feature selection\n",
    "#-----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f0541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Training/test split\n",
    "#-----------\n",
    "\n",
    "# set the random seed\n",
    "np.random.seed(10)\n",
    "\n",
    "\n",
    "# split the data so that it returns 4 values: X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test = train_test_split(...,                 # specify training dataset\n",
    "                                                    ...,                 # specify test dataset\n",
    "                                                    train_size=...,      # specify proportional split for training\n",
    "                                                    test_size=...)       # specify proportional split for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987b8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Validation split\n",
    "#-----------\n",
    "\n",
    "# train_test_split returns 4 values: X_train, X_test, y_train, y_test, so how do we create a 60-20-20 train-validate-test split? \n",
    "X_train, X_validate, y_train, y_validate =  ______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cc8a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Standardization\n",
    "#-----------\n",
    "# Given that we want to only standardize non-dichotomous variables, we need to find a \n",
    "# solution that will loop over only the columns we want to standardize. The code below\n",
    "# identifies all non-dichotomous variables in our dataset and only standardizes those.\n",
    "\n",
    "# load library and create instance of Standard Scaler \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# identify non-dichotomous columns we want to transform\n",
    "columns = list(X_test.select_dtypes(include=['number']).loc[:, X_test.nunique() > 2])\n",
    "\n",
    "# use loop to transform training data for only columns we want to transform\n",
    "for column in columns:\n",
    "    X_train[column] = scaler.fit_transform(X_train[column].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# use loop to transform validation data for only columns we want to transform\n",
    "for column in columns:\n",
    "    X_validate[column] = scaler.fit_transform(X_validate[column].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# use loop to transform test data for only columns we want to transform\n",
    "for column in columns:\n",
    "    X_test[column] = scaler.fit_transform(X_test[column].values.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e762cb",
   "metadata": {},
   "source": [
    "## 4. Train Models\n",
    "\n",
    "In this section, train your models. \n",
    "\n",
    "**Note that if you use Lasso, you will likely need to specify a very low penalty (e.g., an alpha of 0.001) because of convergence problems.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1836726",
   "metadata": {},
   "source": [
    "### 4.1 Describe models\n",
    "\n",
    "Detail the basic logic and assumptions underlying each model, its pros/cons, and why it is a plausible choice for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3818c2",
   "metadata": {},
   "source": [
    "**MODEL DESCRIPTION(S):** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b90463",
   "metadata": {},
   "source": [
    "### 4.2 Train models\n",
    "\n",
    "Train each model in the training set, and be sure to tune hyperparameters if appropriate. Report any relevant summary statistics from the training set, including how well each model fits the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5fb8ca",
   "metadata": {},
   "source": [
    "#### Model 1:  (enter name of model here) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ddd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Model 1 training\n",
    "#-----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988697f",
   "metadata": {},
   "source": [
    "#### Model 2:  (enter name of model here) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea9bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Model 2 training\n",
    "#-----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb1f5e8",
   "metadata": {},
   "source": [
    "#### Model 3:  (enter name of model here) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e375f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Model 3 training\n",
    "#-----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02711ee5",
   "metadata": {},
   "source": [
    "## 5. Validate and Refine Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7738d9",
   "metadata": {},
   "source": [
    "### 5.1 Predict on the validation set\n",
    "Using each of the models you trained, predict outcomes in the validation set. Evaluate how well each model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Predict on validation data\n",
    "#-----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12efdc25",
   "metadata": {},
   "source": [
    "### 5.2 Predict on the test set\n",
    "\n",
    "Now, choose your best performing model of the three, select out unimportant feature(s), retrain the model, and then predict on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58840265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Predict using your best model\n",
    "#-----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79689b2a",
   "metadata": {},
   "source": [
    "### 5.3 Impement a cross-validation approach\n",
    "\n",
    "Finally, implement a cross-validation approach for your best model and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d8c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Run cross-validation\n",
    "#-----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250994bb",
   "metadata": {},
   "source": [
    "## 6. Discussion Questions\n",
    "\n",
    "In this section, insert responses for discussion questions here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2fb212",
   "metadata": {},
   "source": [
    "1. What is bias-variance tradeoff? Why is it relevant to machine learning problems like this one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b143ca1a",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6a596d",
   "metadata": {},
   "source": [
    "2. Define overfitting, and why it matters for machine learning. How can we address it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bfab1f",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e64e982",
   "metadata": {},
   "source": [
    "3. Discuss your analysis in 2-3 paragraphs. Discuss your findings and recommendations. Which counties or regions would you prioritize for the pilot program? Would your answers change based on whether you want to take into account certain features such as race, gender, or age composition in the county? How confident would you be deploying this sort of model in a real-world application – why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dd703e",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
