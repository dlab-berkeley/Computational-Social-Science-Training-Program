---
title: '6-1 Randomized Experiments - Student'
author: ""
date: " `r format(Sys.time(), '%B %d, %Y')`"
output:
  bookdown::gitbook:
    split_by: "none" # other option chapter
    self_contained: true
    config:
      toc:
        collapse: subsection
        scroll_highlight: yes
        before: null
        after: null
      toolbar:
        position: dynamic # or static
      edit : null
      download: yes
      search: yes
      fontsettings:
        theme: white
        family: sans
        size: 2
always_allow_html: true
header-includes:
   - \usepackage{float}
---

```{r intialize, message=FALSE, echo=FALSE, include=FALSE}

#
# initialize
# --------------------------------------------------

# set seed
# ----------
set.seed(13)

# libaries
# ----------
xfun::pkg_attach2(c("tidyverse", 
                    "here", 
                    "knitr",       # for kniting together tables 
                    "kableExtra")) # for styling

# set seed
# ----------
here()

# chunk options ----------------------------------------------------------------
knitr::opts_chunk$set(
  warnings = FALSE,            # prevents warnings from appearing in code chunk
  message = FALSE,             # prevents messages from appearing in code chunk
  results = "markup"           # hide results
)

# prevent scientific notation
# ----------
options(scipen = 999)

```

In this lab, we are going to discuss Randomized Experiments. Causal inference methods can be used for observational data, but it is easier to first consider them in the context of randomized experiments. To begin we are going to created simulated data that we'd be unlikely to encounter in the real world where we give the same individual the treatment and then NOT give them a treatment. We'll then calculate the "true" ***A**verage **T**reatment **E**ffect (**ATE**) and then show how different techniques of applying randomization will give us very close. 

We will be leaning heavily on the `dplyr` library, so I'd encourage you to refer the [dplyr cheat sheet](https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf) to refresh your memory and push your knowledge of how to use the library.

# Simulation

Say that we have a new pain medication, **AspiTyleCedrin,** and we want to determine how effective it is at treating migraines. Ideally, we could observe the entire population's experience of migraines without using the medication, then turn back time and observe the entire population's experience with migraines with the medication, and compare the results to find the casual effect of taking the paid medication. 

So, as a thought experiment, let's simulate this scenario. The following code chunk creates a dataframe containing the following variables:

* `A`: Treatment variable indicating whether the individual  
  - **DID** take AspiTyleCedrin ($A_i = 1$) 
  - **DID NOT** take AspiTyleCedrin ($A_i = 0$)
* `Y_0`: Potential outcome variable ($Y_0 = \text{Migraine} | \text{No AspiTyleCedrin}$) indicating whether if the individual **DID NOT** take AspiTyleCedrin:
  - experience a migraine ($Y_{i0} = 1$) 
  - not experience a migraine ($Y_{i0} = 0$)
* `Y_1`: Potential outcome variable ($Y_1 = \text{Migraine} | \text{AspiTyleCedrin}$) indicating whether if the individual **DID** take AspiTyleCedrin: 
  - experience a migraine ($Y_{i1} = 1$) 
  - not experience a migraine ($Y_{i1} = 0$)
* `W1`: Variable representing sex assigned at birth: 
  - $W1 = 0$ indicating AMAB (assigned male at birth)
  - $W1 = 1$ indicating AFAB (assigned female at birth)
  - $W1 = 2$ indicating an X on the birth certificate, intersex individual, or left blank
* `W2`: Variable representing simplified racial category: 
  - $W2 = 0$ indicating White 
  - $W2 = 1$ indicating Black or African American
  - $W2 = 2$ indicating Non-White Hispanic or Latinx
  - $W2 = 3$ indicating American Indian or Alaska Native
  - $W2 = 4$ indicating Asian
  - $W2 = 5$ indicating Native Hawaiian or Other Pacific Islander
  

**The fundamental problem in causal inference is that we cannot know two different potential states of the same observations** (e.g., if we know $Y_{0}$, we can't know $Y_{1}$ since we cannot give the same individual the treatment and NOT give them a treatment. So this simulation is a hypothetical thought experiment.  

```{r}

#
# create simulated data
# --------------------------------------------------

# set seed - unfortunately seed needs to be set within cell to be reproducible in .Rmd but just once in .R script
set.seed(14)

# set the number of individuals for simulated dataset
# ----------
n = 1e6 # scientific notation for 1,000,000


# create the simulated data frame 
# ----------
# NOTE: Don't worry too much about how we're creating this dataset, this is just for an example.
df <- data.frame(W1 = sample(0:2,                       # sampling from numbers between 0 and 2 (0, 1, and 2)
                             size = n,                  # size of sample that corresponds to n set above
                             replace = TRUE,            # sampling with replacement
                             prob = c(0.49,0.50,0.01)), # probability of each number = MUST SUM TO 1
                 W2 = sample(0:5, 
                             size = n, 
                             replace = TRUE, 
                             prob = c(0.60,0.13,0.19,0.06, 0.015, 0.005))) %>% 
  # use mutate to create remaining variables based on rbernoulli distribution 
  mutate(A = as.numeric(rbernoulli(n, 
                                   p = (0.50 + 0.07*(W1 > 0) + 0.21*(W2 == 0)))),
         Y_0 = as.numeric(rbernoulli(n, 
                                     p = (0.87 + 0.035*(W1 > 0) + 0.05*(W2 > 0)))),
         Y_1 = as.numeric(rbernoulli(n, 
                                     p = (0.34 + 0.035*(W1 > 0) + 0.3*(W2 > 0)))))
       
# view          
head(df)

# calculate summary statistics 
summary(df)

```

## Causal Types

To better understand the potential types that result from taking the treatment or not, let's take a look at a frequency table of the two potential output columns $Y_{0}$ and $Y_{1}$: 

Note that this code chunk uses code from the [kable](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html#HTML_Only_Features) package. This is one common way to make tables that play nice with HTML documents, but there are many others (two other common table creation packages are: [gt](https://gt.rstudio.com) and [modelsummary](https://modelsummary.com))

```{r}

#
# create simulated data
# --------------------------------------------------
# kable documentation : https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html#HTML_Only_Features

# add a column indicating the causal type of each individual
# ----------
df <-                                         # store as "df" object - overwrite
  df %>%                                      # pipe dataframe to mutate function 
  mutate(type = as_factor(4 - (Y_1*2 + Y_0))) # create factor variable "type" using a formula


# Create a frequency table showing how many individuals there are of each causal type
# ----------
df_freq <- 
  df %>%                          # pass dataframe 
  count(Y_1, Y_0, type) %>%       # count by each three variables
  group_by(Y_1) %>%               # group by Y_1 variable 
  mutate(prop = n/nrow(df)) %>%   # calculate the proportion in the table
  arrange(type)                   # sort by type 


# view df_freq table
# ----------
df_freq %>%                         # pass data
  # fancy table making syntax from kable library to make it look nice in html format
  kable() %>%                       # apply kable defaults and make kable object 
  kable_styling(full_width = FALSE)  # make it not span the entire html page 


# Save the proportions of each causal type
# ----------
p_1 <- df_freq$prop[1] # doomed 
p_2 <- df_freq$prop[2] # causal
p_3 <- df_freq$prop[3] # preventive
p_4 <- df_freq$prop[4] # immune

```


This table shows us how many individuals in our population of interest had each of four possible sets of outcomes with and without the use of AspiTyleCedrin, which we may refer to as four different causal "types":

* **Type 1 or "doomed":** These individuals experience a migraine regardless of whether they take AspiTyleCedrin. In our population of interest there are `r df_freq$n[4]` such individuals. The proportion of these individuals out of the entire population of interest is $p_1 \approx$ `r round(p_1, 3)`.
* **Type 2 or "causal":** These individuals experience a migraine if and only if they take AspiTyleCedrin. In our population of interest there are `r df_freq$n[3]` such individuals. The proportion of these individuals out of the entire population of interest is $p_2 \approx$ `r round(p_2, 3)`.
* **Type 3 or "preventive":** These individuals experience a migraine if and only if they do not take AspiTyleCedrin. In our population of interest there are `r df_freq$n[2]` such individuals. The proportion of these individuals out of the entire population of interest is $p_3 \approx$ `r round(p_3, 3)`.
* **Type 4 or "immune":**  These individuals do not experience a migraine regardless of whether they take AspiTyleCedrin. In our population of interest there are `r df_freq$n[1]` such individuals. The proportion of these individuals out of the entire population of interest is $p_4 \approx$ `r round(p_4, 3)`.


# Causal Parameters

## Individual-level Treatment Effect (ITE)

The Individual-level Treatment Effect is simply the difference between the potential outcomes for a particular individual $i$, that is:

$$\text{ITE}_i = Y_{i1} - Y_{i0}$$
**<span style="color:blue;">QUESTION 1:</span>** Use the `mutate()` function to add a column to `df` named `ITE` which contains the individual-level treatment effect for each row.

**WRITE YOUR ANSWER HERE ...**

```{r}

```


## Average Treatment Effect (ATE)

So, a common causal parameter of interest is the ***A**verage **T**reatement **E**ffect (**ATE**), which is the average difference in the pair of potential outcomes averaged over the entire population of interest (at a particular moment in time). In other words, it is just the average (or expected value) of the individual-level treatment effect.

$$\text{ATE} = E[Y_{i1} - Y_{i0}]$$
**<span style="color:blue;">QUESTION 2:</span>** Use the `ITE` column you just added to `df` to find the average treatment effect of AspiTyleCedrin on migraines in this population and assign it to the variable name `ATE`.

**WRITE YOUR ANSWER HERE ...**

```{r}

```

**<span style="color:blue;">ANSWER 2:</span>** You should have gotten a result around $\text{ATE} \approx -0.43$. This means that for our entire population of interest, AspiTyleCedrin decreases migraines by about 43% on average.

Notice that, since the expected value is a linear operator:

$$\text{ATE} = E[Y_{i1} - Y_{i0}] =  E[Y_{i1}] - E[Y_{i0}]$$
**<span style="color:blue;">QUESTION 3:</span>** So, with a bit of math, the ATE is simply $E[Y_{i1}] - E[Y_{i0}]$. Confirm this using `R`.

**WRITE YOUR ANSWER HERE ...**

```{r}



```

Let us consider this in terms of the proportions of causal types above, $p_1, p_2, p_3, p_4$. The expected value of $Y_{i1}$ is simply the proportion of the entire population of interest that experiences migraines if everyone takes AspiTyleCedrin. So, it is really the sum of both types that experience migraines when everyone takes AspiTyleCedrin (i.e. the sum of the "doomed" and the "causal" groups):

$$E[Y_{i1}] = p_1 + p_2$$
Similarly, the expected value of $Y_{i0}$ is simply the proportion of the entire population of interest that experiences migraines if nobody takes AspiTyleCedrin, so it is really the sum of both types that experience migraines when nobody takes AspiTyleCedrin (i.e. the sum of the "doomed" and the "preventive" groups):

$$E[Y_{i0}] = p_1 + p_3$$
Therefore, the average treatment effect can be re-written as follows:

\begin{align*}
\text{ATE} &= E[Y_{i1} - Y_{i0}] \\
&=  E[Y_{i1}] - E[Y_{i0}] \\
&= (p_1 + p_2) - (p_1 + p_3)  \\
&= p_2 - p_3
\end{align*}

Or rather, the average treatment effect is equivalent to the difference between the proportions of the "causal" and "preventive" groups.

**<span style="color:blue;">QUESTION 4:</span>** Again, confirm this using `R`.

**WRITE YOUR ANSWER HERE ...**

```{r}

```

## Average Treatment Effect on the Treated (ATT)

Another common causal parameter that we may be interested in is the **A**verage **T**reatment **E**ffect on the **T**reated (**ATT**) which, instead of considering the entire population of interest, only considers those who were actually treated. 

**<span style="color:blue;">QUESTION 5:</span>**  Use the `filter()` function to create a new data frame `df_treat` which is a subset of `df` containing only those who receive the treatment. Then find the average treatment effect on the treated of AspiTyleCedrin on migraines in this population and assign it to the variable name `ATT`.

**WRITE YOUR ANSWER HERE ...**

```{r}

```


## Heterogeneous Treatment Effects

In many cases, treatment effects may look very different among different groups, so it may be worth considering whether there are heterogeneous treatment effects within your population of interest. For example, say we have reason to suspect AspiTyleCedrin is more or less effective among different racial groups.

**<span style="color:blue;">QUESTION 6:</span>** Use the `group_by()` and `summarize()` functions to calculate separate average treatment effects for each race category. Discuss your results.

**WRITE YOUR ANSWER HERE ...**

```{r}

```

We see a clear difference in the average treatment effect for White individuals than other racial groups. The average treatment effect of AspiTyleCedrin among White people is an approximately 53% decrease in migraine incidence, whereas for people of color it is only an approximately 28% decrease in migraine incidence. AspiTyleCedrin appears to be less effective for people of color than it is for White people.

# Experimental Designs

Now, all of the above calculations were made using complete information about the experience of migraines for the entire population of interest both with and without the use of AspiTyleCedrin. In reality, of course cannot have this information--it's impossible to know both states. Instead of knowing the potential values in `Y_0` and `Y_1`, we would instead only have one of those two possible outcomes,`Y_i`, for each individual.

For example, using information on the treatment (the `A` column), we can create an observed outcome column `Y_obs`. This will give us dataset that is more realistic--where we only have one outcome per individual.

```{r}


# create a new dataframe with Y_obs that is more realistic
# ----------
df <- 
  df %>%                                               # pass data 
  mutate(Y_obs = as.numeric((A & Y_1) | (!A & Y_0)))   # boolean equalities

  # SUMMARY :
  # if A & Y_1 are true (equal to 1) then 1 
  # A != 1 (so 0) and Y_0 = 1, then 1
  # all else zero 
  #

```

Now we consider a frequency table of `A` versus `Y_obs`:

```{r}

# create frequency table only looking at A (treatment) and Y_obs (observed the outcome)
# ----------
df %>%                  # pass data
  select(A, Y_obs) %>%  # keep only A (treatment) and Y_obs (observed the outcome)
  ftable()              # create a frequency table

```

Or, we can lay it out slightly differently, as below. Confirm for yourself that these two tables are equivalent.

```{r}

# same as ftable but long 
# ----------
df %>%                    # pass data
  group_by(A, Y_obs) %>%  # group by whether they received treatment and the observed outcome
  count()                 # count 

```

The above table is the only information (aside from covariates, etc.) that we would be able to obtain in reality, even if we were able to sample the entire population of interest, as we have here. 

How does this table relate to the different causal types? Let's consider this same table, but separate out counts by causal type.

```{r}

# include type in calculations 
# ----------
df_freq_obs_type <- 
  df %>%                       # pass data
  group_by(A, Y_obs, type) %>% # group by the three variable we want
  count()                      # count

df_freq_obs_type

```

Thus, we can see that each of the cells in the original table is actually composed of a mixture of two different causal types each. Conversely, each causal type makes up part of the cell count for two different arrangements of `A` and `Y_obs`. In other words:

* Those that DID NOT take AspiTyleCedrin and DID NOT experience a migraine could be either *"causal"* or *"immune"*.
* Those that DID NOT take AspiTyleCedrin and DID experience a migraine could be either *"doomed"* or *"preventive"*.
* Those that DID take AspiTyleCedrin and DID NOT experience a migraine could be either *"preventive"* or *"immune"*.
* Those that DID take AspiTyleCedrin and DID experience a migraine could be either *"doomed"* or *"causal"*.

In reality, for each of these categories **we have no way of knowing which is which** of the corresponding two causal types. In practice, this makes it difficult to estimate the average treatment effect. For that reason, there are a number of different estimators used to estimate this parameter, but for now we will consider a very naive one, that is:

$$\hat{\text{ATE}} = E[Y_i |A_i = 1] - E[Y_i|A_i = 0]$$
**<span style="color:blue;">QUESTION 7:</span>** Use the `group_by()` and `summarize()` functions to find the estimated average treatment effect using the `A` and `Y_obs` columns. Compare this result to the actual ATE calculated earlier.

**WRITE YOUR ANSWER HERE ...**

```{r}

```

## Independence Assumption and Random Assignment

If we consider this estimator in terms of the causal groups, we note that $E[Y|A = 1]$ is composed of parts of causal types 1 and 2, and $E[Y|A = 0]$ is composed of parts of causal types 1 and 3. Or rather:

\begin{align*}
\hat{\text{ATE}} &= E[Y_i|A_i = 1] - E[Y_i|A_i = 0] \\
&= \frac{(?n_1 + ?n_2)}{(?n_1 + ?n_2 + ?n_3 + ?n_4)} - \frac{(?n_1 + ?n_3)}{(?n_1 + ?n_2 + ?n_3 + ?n_4)}
\end{align*}

Where each question mark is some unknown fraction, and $n_{\text{type}}$ is the number of individuals of that causal type. If we could somehow be assured that all of the unknown fractions were equivalent, then the following would also be true:

\begin{align*}
\hat{\text{ATE}} &= \frac{(?n_1 + ?n_2)}{(?n_1 + ?n_2 + ?n_3 + ?n_4)} - \frac{(?n_1 + ?n_3)}{(?n_1 + ?n_2 + ?n_3 + ?n_4)} \\
&= \frac{?(n_1 + n_2)}{?(n_1 + n_2 + n_3 + n_4)} - \frac{?(n_1 + n_3)}{?(n_1 + n_2 + n_3 + n_4)} \\
&=   \frac{(n_1 + n_2)}{(n_1 + n_2 + n_3 + n_4)} - \frac{(n_1 + n_3)}{(n_1 + n_2 + n_3 + n_4)}\\
&=   \left( \frac{n_1}{(n_1 + n_2 + n_3 + n_4)} + \frac{n_2}{(n_1 + n_2 + n_3 + n_4)} \right) - \left( \frac{n_1}{(n_1 + n_2 + n_3 + n_4)} + \frac{n_3}{(n_1 + n_2 + n_3 + n_4)} \right) \\
&=   (p_1 + p_2) - (p_1 + p_3)  \\
&=   p_2 - p_3  \\
\end{align*}

And thus we could rest assured that our estimator $\hat{\text{ATE}}$ is actually equivalent to the parameter $\text{ATE}$. 

Unfortunately, we've seen that these assumptions do not hold for our observed data. However, in an experimental design, we can control the assignment of the intervention variable `A` (AspiTyleCedrin, in our case). If we are somehow able to ensure that the assignment of the intervention `A` is **independent** of the causal type, then these assumptions will hold.

**<span style="color:blue;">QUESTION 8:</span>** Explain why this is true. That is, show mathematically that if `A` and `type` are independent, then all of the question marks above are equivalent.

**WRITE YOUR ANSWER HERE ...**


**<span style="color:blue;">QUESTION 9:</span>** Briefly explain why we can ensure this independence assumption by randomly assigning individuals to take AspiTyleCedrin.

**WRITE YOUR ANSWER HERE ...**


## Completely Randomized Designs

There are different methods of randomization. The most conceptually simple is complete randomization. That is, randomly assigning the intervention for the entire sample.

We will simulate this by creating a new assignment variable `A_comp`, and a new outcome variable `Y_comp`:

```{r}

# completely randomize treatment
# ----------
df <-                                                           # save object
  df %>%                                                        # pass data
  mutate(A_comp = as.numeric(rbernoulli(n, p=0.5)),             # create completely randomized assignment
         Y_comp = as.numeric((A_comp & Y_1) | (!A_comp & Y_0))) # create completely randomized outcome

```

**<span style="color:blue;">QUESTION 10:</span>** Create a new frequency table for `A_comp` and `Y_comp` (of whichever layout you prefer from above) and then assign the new estimated average treatment effect to the variable `ATE_comp`. Then, compare your result to the true parameter `ATE`.

**WRITE YOUR ANSWER HERE ...**

```{r}

```

## Cluster Randomized Designs

Another method of randomization is cluster randomization. In this method, individuals are broken up into cluster, in which all members of the same cluster are assigned the same treatment (i.e. all $A = 0$ or all $A = 1$) but *clusters* are randomized, such that each cluster has an equal chance of being assigned to the intervention. 

To simulate this, we will create a new `cluster` column which contains a number 1 through 100 to indicate which of 100 clusters each individual belongs to.

```{r}

# create cluster for randomization 
# ----------
df <- 
  df %>%                                     # pass data 
  mutate(cluster = rep(1:100, each = n/100)) # assign individuals to clusters

```


**<span style="color:blue;">QUESTION 11:</span>** Create a new assignment variable `A_clus`, and a new outcome variable `Y_clus`, making sure that all members of a given cluster have the same value for `A_clus`. Then, create a new frequency table for `A_clus` and `Y_clus`, assign the new estimated average treatment effect to the variable `ATE_clus`, and briefly compare your result to the true parameter `ATE`.


```{r}

# cluster randomization assignment
# ----------
df <- 
  df %>%                                                                   # pass data
  mutate(A_clus = rep(as.numeric(rbernoulli(100, p =0.5)), each = n/100),  # create cluster assignment
         Y_clus = as.numeric((A_clus & Y_1) | (!A_clus & Y_0)))            # create new outcome

```

**WRITE YOUR ANSWER HERE ...**

```{r}

```


Note that this example is simply clustering individuals by location in our dataframe, and thus individuals in a given cluster are not statistically more similar to each other than they are to individuals in other clusters. In reality, clusters usually are statistically more similar to each other than to individuals in other clusters (for example, perhaps each cluster represents patients of a particular hospital). This is in fact a common reason *why* a cluster randomized design is used.

## Block Randomized Designs

Block randomized designs can be thought of as the reverse of cluster designs. That is, individuals are broken up into blocks, and then randomization occurs *within each block*. One example of this might be in a study where individuals are interviewed and then randomized in chunks at different points in time. This is common in longitudinal studies where you interview a few people at a time but need to assign them to treatment or control so they can proceed with the study. 

To simulate this so we can see how this works, we will create a new `block` column which contains a number 1 through 20 to indicate which of 20 blocks each individual belongs to.

```{r}

# create block for randomization 
# ----------
df <- 
  df %>%      
  mutate(block = rep(1:20, each = n/20))

```

**<span style="color:blue;">QUESTION 12:</span>** Create a new assignment variable `A_bloc`, and a new outcome variable `Y_bloc`. Then, create a new frequency table for `A_bloc` and `Y_bloc`, assign the new estimated average treatment effect to the variable `ATE_bloc`, and briefly compare your result to the true parameter `ATE`. Hint: Before creating `A_bloc`, take a look at the help page for the `replicate()` function.

**WRITE YOUR ANSWER HERE ...**

```{r}

```


Note again that this example is simply blocking individuals by location in our dataframe, and thus individuals in a given block are not statistically more similar to each other than they are to individuals in other blocks. In reality, blocks usually are statistically more similar to each other than to individuals in other blocks (for example, perhaps each block represents a geographical region).

# Statistical Tests of Difference

A common statistical question (in fact, often the actual research question of interest) is whether some variable in our dataset (usually the dependent or outcome variable) varies by one or more other (usually independent) variables. 

There are many different statistical tests to try to answer such questions, and choosing among these tests depends on the context. In particular, choosing a test depends upon the types of variables you are comparing, and the assumptions you are able to make about the underlying distributions of the continuous variables. The following table is a quick guide for a few different scenarios. The first column lists tests that can be used when assuming Normality of any continuous variables, the second column lists non-paramtric tests that do not make distributional assumptions.

| Variable Types                                           | Normal                         | Non-Parametric               |
|----------------------------------------------------------|--------------------------------|------------------------------|
| 2 Categorical                                           | χ² Test                        | χ² Test                       |
| 1 Categorical (2 levels), 1 Continuous                  | t-Test (means)                 | Rank-Sum Test (medians)       |
| 1 Categorical (>2 levels), 1 Continuous OR >1 Categorical, 1 Continuous | ANOVA a.k.a. F-Test (means) | Kruskal-Wallis Test (medians)|
| 2 Continuous                                            | Pearson Correlation            | Spearman Correlation          |

For our dataset, if we want to explore whether the experience of migraines varies among those who took AspiTyleCedrin or not, or among simplified racial groups or sex assigned at birth, we could use either $\chi^2$ or ANOVA, since these are all categorical variables, some with more than two levels. Let's take a look at both methods using the completely randomized dataset.

## $\chi^2$ Test

Let's first do a simple $\chi^2$ test just to confirm that there is evidence of a difference in migraine experience among those who took AspiTyleCedrin or not.

```{r}
chisq.test(table(df$Y_comp, df$A_comp))
```

Since the p-value from this test is so small, we see that there is evidence that migraine incidence is indeed different between these two groups.

Now let's see if migraine experience is different among different sexes assigned at birth.

```{r}
chisq.test(table(df$Y_comp, df$W1))
```

Yet again we see a very small p-value, meaning that we do indeed have evidence of a difference in migraine experience among these groups.

**<span style="color:blue;">QUESTION 13:</span>** Run a $\chi^2$ test to see if there is a difference in migraine experience among simplified racial groups. Briefly discuss your results.

**WRITE YOUR ANSWER HERE ...**

```{r}

```

Yet again we see a very small p-value, meaning that we do indeed have evidence of a difference in migraine experience among these groups.

## Analysis of Variance (ANOVA) a.k.a. $F$-Test

Our outcome of migraine experience is categorical, but we can still use methods like ANOVA which can handle continuous outcomes. This is especially useful if we wish to use two-way ANOVA, which we will in a moment. First, let's replicate the $\chi^2$ tests we just performed using one-way ANOVA.

```{r}

# Migraine difference between AspiTyleCedrin use and not
summary(aov(Y_comp ~ A_comp, data = df)) 

# Migraine difference among sexes assigned at birth
summary(aov(Y_comp ~ W1, data = df)) 

# Migraine difference among simplified racial groups
summary(aov(Y_comp ~ W2, data = df)) 


```

Note that each of these p-values are again quite small, indicating evidence for a difference in migraine experience among the different groups.

Two-way ANOVA allows us to test for a difference in the dependent (outcome) variable among multiple independent variables, including interaction terms. For example, if we want to test for a difference in migraine experience among both sex assigned at birth and simplified racial group, we can do the following.

```{r}
summary(aov(Y_comp ~ W1*W2, data = df)) 
```

Note that we again found significant p-values for each of the two variables sex assigned at birth and simplified racial group, but we did not find a significant p-value for the interaction term. Note that this does not give us evidence that there is no interaction, it just does not give us evidence that there is interaction.

**<span style="color:blue;">QUESTION 14:</span>** Repeat the one-way and two-way ANOVA tests using the observed data (`A` and `Y_obs`) instead of the completely randomized data (`A_comp` and `Y_comp`). Briefly discuss your results.


**WRITE YOUR ANSWER HERE ...**

```{r}

```
