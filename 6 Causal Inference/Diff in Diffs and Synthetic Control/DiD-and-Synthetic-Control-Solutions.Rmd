---
title: "Diff in Diffs and Synthetic Control"
output: pdf_document
---

```{r}
# Install packages 
if (!require("pacman")) install.packages("pacman")

#install.packages("devtools") # download developer tools package 
library(devtools) 

devtools::install_github("ebenmichael/augsynth")

pacman::p_load(# Tidyverse packages including dplyr and ggplot2 
               tidyverse,
               ggthemes,
               augsynth)

# you might need to update the 'rlang' package here if you've had r for a while and haven't updated 


set.seed(44)
```

# Introduction

In this lab we will explore difference-in-differences estimates and a newer extension, synthetic control. The basic idea behind both of these methods is simple - assuming two units are similar in a pre-treatment period and one undergoes treatment while the other stays in control, we can estimate a causal effect by taking three differences. First we take the difference between the two in the pre-treatment period, then take another difference in the post-treatment period. Then we take a difference between these two differences (hence the name difference in differences). Let's see how this works in practice!

# Basic DiD

We'll use the kansas dataset that comes from the `augsynth` package. Our goal here is to estimate the effect of the 2012 Kansas tax cuts on state GDP. Let's take a look at our dataset:

```{r}
data(kansas)
summary(kansas)
```

We have a lot of information here! We have quarterly state GDP from 1990 to 2016 for each U.S. state, as well as some other covariates. Let's begin by adding a treatment indicator to Kansas in Q2 2012 and onward.

```{r}
kansas <- kansas %>%
  select(year, qtr, year_qtr, state, treated, gdp, lngdpcapita, fips) %>% 
  mutate(treatment = ifelse(state == "Kansas" & year_qtr >= 2012.50,
                            1,
                            0))
head(kansas)
```

One approach might be to compare Kansas to itself pre- and post-treatment. If we plot state GDP over time we get something like this:

```{r}
kansas %>%
  filter(state == 'Kansas') %>%
  ggplot() +
  geom_point(aes(x = year_qtr, y = lngdpcapita)) +
  geom_vline(xintercept = 2012.5) +
  theme_fivethirtyeight() +
  theme(axis.title = element_text()) +
  ggtitle('Kansas State GDP Per Capita Over Time') +
  xlab('Year-Quarter') +
  ylab('State GDP Per Capita')
  
```

**Question**: Looks like GDP went up after the tax cut! What is the problem with this inference?

**Solution**: It looks like GDP went up after the tax cut, but we have no way of telling whether it went up because of the tax cut or went up because it would have otherwise. In short, we need to compare the treated Kansas to a counterfactual for if taxes weren't cut.

Ideally, we would like to compare treated Kansas to control Kansas. Because of the fundamental problem of causal inference, we will never oberserve both of these conditions though. The core idea behind DiD is that we could instead use the fact that our treated unit was similar to a control unit, and then measure the differences between them. Perhaps we could choose neighboring Colorado:

```{r}
kansas %>%
  filter(state %in% c("Kansas","Colorado")) %>%
  filter(year_qtr >= 2012.5 & year_qtr<= 2012.75) %>%
  ggplot() + 
  geom_point(aes(x = year_qtr, 
                 y = lngdpcapita, 
                 color = state)) +
  geom_line(aes(x = year_qtr, 
                y = lngdpcapita, 
                color = state)) +
  theme_fivethirtyeight() +
  theme(axis.title = element_text()) +
  ggtitle('Colorado and Kansas GDP \n before/after Kansas tax cut') +
  xlab('Year-Quarter') +
  ylab('State GDP')
```

This is basically what [Card-Krueger (1994)](https://davidcard.berkeley.edu/papers/njmin-aer.pdf) did measuring unemployment rates among New Jersey and Pennsylvania fast food restaurants. 

**Challenge**: Try writing a simple DiD estimate using dplyr/tidyr (use subtraction instead of a regression):

```{r}
# kansas-colorado
kc <- kansas %>%
  filter(state %in% c("Kansas","Colorado")) %>%
  filter(year_qtr >= 2012.5 & year_qtr<= 2012.75) 

# pre-treatment difference

pre_diff <- kc %>%
  filter(year_qtr == 2012.5) %>%
  select(state, 
         lngdpcapita) %>%
  spread(state, 
         lngdpcapita) %>%
  summarise(Colorado - Kansas)
  
# post-treatment difference

post_diff <- kc %>%
  filter(year_qtr == 2012.75) %>%
  select(state, 
         lngdpcapita) %>%
  spread(state, 
         lngdpcapita) %>%
  summarise(Colorado - Kansas)

# diff-in-diffs

diff_in_diffs <- post_diff - pre_diff
diff_in_diffs
```

Looks like our treatment effect is about .003 (in logged thousands dollars per capita). Again this is the basic idea behind Card-Krueger.

**Question**: Why might there still be a problem with this estimate?

**Answer**: We just assumed that Colorado was similar to Kansas because they are neighbors - we don't really have evidence for this idea.

# Parallel Trends Assumptions

One of the core assumptions for difference-in-differences estimation is the "parallel trends" or "constant trends" assumption. Essentially, this assumption requires that the difference between our treatment and control units are constant in the pre-treatment period. Let's see how Kansas and Colorado do on this assumption:

```{r}
kansas %>%
  filter(state %in% c("Kansas","Colorado")) %>%
  ggplot() + 
  geom_point(aes(x = year_qtr, 
                 y = lngdpcapita, 
                 color = state)) +
  geom_line(aes(x = year_qtr, 
                y = lngdpcapita, 
                color = state)) +
  geom_vline(aes(xintercept = 2012.5)) +
  theme_fivethirtyeight() +
  theme(axis.title = element_text()) +
  ggtitle('Colorado and Kansas GDP \n before/after Kansas tax cut') +
  xlab('Year-Quarter') +
  ylab('State GDP')
```

The two lines somewhat move together, but the gap does grow and shrink at various points over time. The most concerning part here is that the gap quickly shrinks right before treatment. What do we do if we do not trust the parallel trends assumption? Perhaps we pick a different state.

**Challenge**: Choose another state that you think would be good to try out, and plot it alongside Kansas and Colorado.

```{r}
kansas %>%
  filter(state %in% c("Kansas","Colorado", "Missouri")) %>%
  ggplot() + 
  geom_point(aes(x = year_qtr, 
                 y = lngdpcapita, 
                 color = state)) +
  geom_line(aes(x = year_qtr, 
                y = lngdpcapita, 
                color = state)) +
  geom_vline(aes(xintercept = 2012.5)) +
  theme_fivethirtyeight() +
  theme(axis.title = element_text()) +
  ggtitle('Colorado and Kansas GDP \n before/after Kansas tax cut') +
  xlab('Year-Quarter') +
  ylab('State GDP')
```

**Question**: Would you pick Colorado or your choice? be the more plausible control unit in this case? Why?

**Solution**: There is a good argument for both of them (Missouri in this case). However, the gap between Colorado and Kansas closes quickly before the treatment period, and similarly it grows between between Kansas and Missouri at the same point.

Selecting comparative units this way can be hard to justify theoretically, and sometimes we do not have a good candidate. What can we do then? This is where synthetic control comes in.

# Synthetic Control

Synthetic control is motivated by the problem of choosing comparison units for comparative case studies. It aims to create a "synthetic" version of the treatment unit by combining and weighting covariates from other units ("donors"). In this case, we would construct a synthetic Kansas by creating a weighted average of the other 49 U.S. states. Ideally, the synthetic unit would match the treatment unit in the pre-treatment periods.

For constructing a synthetic control, we are going to use the [`augsynth`](https://github.com/ebenmichael/augsynth) library. The basic syntax for this library is:

`augsynth(outcome ~ trt, unit, time, t_int, data)`

```{r}
syn <- augsynth(lngdpcapita ~ treated, state, year_qtr, kansas,
                progfunc = "None", scm = T)

summary(syn)
```

We can use the built in plot function to see how Kansas did relative to synthetic Kansas:

```{r}
plot(syn)
```

We can see which donors contributed the most to the synthetic Kansas:

```{r}
# Convert weights to dataframe
data.frame(syn$weights) %>%
  # change index to a column
  tibble::rownames_to_column('State') %>%
  ggplot() +
  # stat = identity to take the literal value instead of a count for geom_bar()
  geom_bar(aes(x = State, 
               y = syn.weights),
           stat = 'identity') +
  theme_fivethirtyeight() +
  theme(axis.title = element_text(),
        axis.text.x = element_text(angle = 90)) +
  ggtitle('Synthetic Control Weights') +
  xlab('State') +
  ylab('Weight') 
```

Surprisingly, only a few units ended up contributing! Let's take a closer look at the ones that did:

```{r}
data.frame(syn$weights) %>%
  tibble::rownames_to_column('State') %>%
  filter(syn.weights > 0) %>%
  ggplot() +
  geom_bar(aes(x = State, 
               y = syn.weights),
           stat = 'identity') +
  theme_fivethirtyeight() +
  theme(axis.title = element_text(),
        axis.text.x = element_text(angle = 90)) +
  ggtitle('Synthetic Control Weights') +
  xlab('State') +
  ylab('Weight') 
  
```

# Synthetic Control Augmentation

The main advantage of the `asynth` package is that it allows for ["augmented synthetic control"](https://arxiv.org/abs/1811.04170). One of the main problems with synthetic control is that if the pre-treatment balance between treatment and control outcomes is poor, the estimate is not valid. Specifically, they advocate for using [L2 imbalance](https://en.wikipedia.org/wiki/Ridge_regression#:~:text=Ridge%20regression%20is%20a%20method,econometrics%2C%20chemistry%2C%20and%20engineering.), which he first encountered as the penalty that ridge regression uses. L2 uses "squared magnitude" of the coefficient to penalize a particular feature.

```{r}
# Aniket's method for getting the underlying data
syn_sum <- summary(syn)

kansas_synkansas <- kansas %>%
  filter(state == "Kansas") %>%
  bind_cols(difference = syn_sum$att$Estimate) %>%
  mutate(synthetic_kansas = lngdpcapita + difference)

# Plot

kansas_synkansas %>%
  ggplot() +
  geom_point(aes(x = year_qtr, 
                 y = lngdpcapita, 
                 color = 'Kansas')) +
  geom_line(aes(x = year_qtr, 
                y = lngdpcapita, 
                color = 'Kansas')) +
  geom_point(aes(x = year_qtr, 
                 y = synthetic_kansas, 
                 color = 'Synthetic Kansas')) +
  geom_line(aes(x = year_qtr, 
                y = synthetic_kansas, 
                color = 'Synthetic Kansas')) +
  scale_color_manual(values = c('Kansas' = 'red', 'Synthetic Kansas' = 'blue')) +
  geom_vline(aes(xintercept = 2012.5)) +
  theme_fivethirtyeight() +
  theme(axis.title = element_text()) +
  ggtitle('Kansas and Synthetic Kansas') +
  xlab('Year-Quarter') +
  ylab('State GDP Per Capita')
```

**Question**: How does pre-treatment matching between Kansas and Synthetic Kansas look here?

**Answer**: Pretty good! We may not need to augment this synthetic control, though let's try anyway.

```{r}
ridge_syn <- augsynth(lngdpcapita ~ treated, state, year_qtr, kansas,
                progfunc = "ridge", scm = T)

summary(ridge_syn)
```

Let's look at the weights:

```{r}
data.frame(ridge_syn$weights) %>%
  tibble::rownames_to_column('State') %>%
  ggplot() +
  geom_bar(aes(x = State, y = ridge_syn.weights),
           stat = 'identity') +
  theme_fivethirtyeight() +
  theme(axis.title = element_text(),
        axis.text.x = element_text(angle = 90)) +
  ggtitle('Synthetic Control Weights') +
  xlab('State') +
  ylab('Weight') 
```

Notice how with the ridge augmentation, some weights are allowed to be negative now. Now let's go ahead and plot the ridge augmented synthetic Kansas alongside Kansas and synthetic Kansas:

```{r}
ridge_sum <- summary(ridge_syn)

kansas_synkansas_ridgesynkansas <- kansas_synkansas %>%
  bind_cols(ridge_difference = ridge_sum$att$Estimate) %>%
  mutate(ridge_synthetic_kansas = lngdpcapita + ridge_difference)

kansas_synkansas_ridgesynkansas %>%
  ggplot() +
  geom_point(aes(x = year_qtr, 
                 y = lngdpcapita, 
                 color = 'Kansas')) +
  geom_line(aes(x = year_qtr, 
                y = lngdpcapita, 
                color = 'Kansas')) +
  geom_point(aes(x = year_qtr, 
                 y = synthetic_kansas, 
                 color = 'Synthetic Kansas')) +
  geom_line(aes(x = year_qtr, 
                y = synthetic_kansas, 
                color = 'Synthetic Kansas')) +
  geom_point(aes(x = year_qtr, 
                 y = ridge_synthetic_kansas, 
                 color = 'Ridge Synthetic Kansas')) +
  geom_line(aes(x = year_qtr, 
                y = ridge_synthetic_kansas, 
                color = 'Ridge Synthetic Kansas')) +
  scale_color_manual(values = c('Kansas' = 'red', 
                                'Synthetic Kansas' = 'blue',
                                'Ridge Synthetic Kansas' = 'green')) +
  geom_vline(aes(xintercept = 2012.5)) +
  theme_fivethirtyeight() +
  theme(axis.title = element_text()) +
  ggtitle('Kansas, Synthetic Kansas, Ridge Synthetic Kansas') +
  xlab('Year-Quarter') +
  ylab('State GDP Per Capita')
```

These all seem pretty good! Like we thought, augmentation did not necessarily improve the matches in this particular dataset. We can check the two L2 imbalances and see that we have reduced the overall imbalance a bit with our ridge model:

```{r}
print(syn$l2_imbalance)
print(ridge_syn$l2_imbalance)
```

Finally, we can add covariates to our model if we would like:

```{r}
data(kansas)

covsyn <- augsynth(lngdpcapita ~ treated | lngdpcapita + log(revstatecapita) +
                                           log(revlocalcapita) + log(avgwklywagecapita) +
                                           estabscapita + emplvlcapita,
                   fips, year_qtr, kansas,
                   progfunc = "ridge", scm = T)

summary(covsyn)
```

## Staggered Adoption

The last technique we'll look at is "staggered adoption" of some policy. In the original Hainmueller paper, states that already had similar cigaratte taxes were discarded from the donor pool to create a synthetic California. But what if we were interested in the effect of a policy overall, for every unit that adopted treatment? The problem is, these units all choose to adopt treatment at different times. We could construct different synthetic controls for each one, or we can use a staggered adoption approach.

To explore this question, we'll continue using the `augsynth` package's vignette. This time we will load a dataset that examines the effect of states instituting mandatory collective bargaining agreements.

```{r}
collective_bargaining <- read_delim("https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/WGWMAV/3UHTLP", delim = '\t')

head(collective_bargaining)
```
The main variables we'll use here are:

The dataset contains several important variables that we'll use:

- `year`, `State`: The state and year of the measurement
- `YearCBrequired`: The year that the state adopted mandatory collective bargaining
- `lnppexpend`: Log per pupil expenditures in 2010 dollars

Let's do some preprocessing before we estimate some models. We're going to remove DC and Wisconsin from the analysis and cabin our dataset to 1959 - 1997. Finally, we'll add a treatment indicator `cbr` which takes a 1 if the observation was a treated state after it adopted mandatory collective bargaining, or a 0 otherwise: 

```{r}
collective_bargaining_clean <- collective_bargaining %>%
    filter(!State %in% c("DC", "WI"),
           year >= 1959, 
           year <= 1997) %>%
    mutate(YearCBrequired = ifelse(is.na(YearCBrequired), 
                                   Inf, YearCBrequired),
           cbr = 1 * (year >= YearCBrequired))
```

We're ready to start estimating a model! To do this, we use the `multisynth()` function that has the following signature:

```
mutltisynth(outcome ~ treatment, unit, time, nu, data,  n_leads)
```

The key parameters here are `nu` and `n_leads`. Staggered adoption uses multi-synthetic control which essentially pools together similar units and estimates a synthetic control for each pool. `nu` determines how much pooling to do. A value of 0 will fit a separate synthetic control for each model, whereas a value of 1 will pool all units together. Leaving this argument blank with have `augsynth` search for the best value of `nu` that minimizes L2 loss. `n_leads` determines how many time periods to estimate in the post-treatment period.

```{r}
# with a choice of nu
ppool_syn <- multisynth(lnppexpend ~ cbr, State, year, 
                        nu = 0.5, collective_bargaining_clean, n_leads = 10)
# with default nu
ppool_syn <- multisynth(lnppexpend ~ cbr, State, year, 
                        collective_bargaining_clean, n_leads = 10)

print(ppool_syn$nu)

ppool_syn
```

After you've fit a model that you like, use the `summary()` function to get the ATT and balance statistics.

```{r}
ppool_syn_summ <- summary(ppool_syn)
```

Next, plot the estimates for each state as well as the average average treatment effect (so average for all treated states). Try to do this with `ggplot()` instead of the built-in plotting function (hint: how did we get the dataframe with the estimates before?)

```{r}
ppool_syn_summ$att %>%
  ggplot(aes(x = Time, y = Estimate, color = Level)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = 0) +
  theme_fivethirtyeight() +
  theme(axis.title = element_text(),
        legend.position = "bottom") +
  ggtitle('Synthetic Controls for State Collective Bargaining') +
  xlab('Time') +
  ylab('Expenditure on Pupil Estimate')
```

```{r}
ppool_syn_summ$att %>%
  ggplot(aes(x = Time, y = Estimate, color = Level)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = 0) +
  theme_fivethirtyeight() +
  theme(axis.title = element_text(),
        legend.position = 'None') +
  ggtitle('Synthetic Controls for State Collective Bargaining') +
  xlab('Time') +
  ylab('Expenditure on Pupil Estimate') +
  facet_wrap(~Level)
```

We can also combine our observations into "time cohorts" or units that adopted treatment at the same time. Try adding `time_cohort = TRUE` to your multisynth function and see if your estimates differ. Plot these results as well.

```{r}
ppool_syn_time <- multisynth(lnppexpend ~ cbr, State, year,
                        collective_bargaining_clean, n_leads = 10, time_cohort = TRUE)

ppool_syn_time_summ <- summary(ppool_syn_time)

ppool_syn_time_summ
```

```{r}
ppool_syn_time_summ$att %>%
  ggplot(aes(x = Time, y = Estimate, color = Level)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = 0) +
  theme_fivethirtyeight() +
  theme(axis.title = element_text(),
        legend.position = 'None') +
  ggtitle('Synthetic Controls for State Collective Bargaining') +
  xlab('Time') +
  ylab('Expenditure on Pupil Estimate') +
  facet_wrap(~Level)
```

Finally, we can add in augmentation. Again augmentation essentially adds a regularization penalty to the synthetic control weights. In the multisynth context, you may especially want to do this when the pre-treatment fit is poor for some of your units. There are a couple of different options for augmentation. One is to specify `fixed_effects = TRUE` in the multsynth call, and this will estimate unit fixed effects models after de-meaning each unit. We can also specify a `n_factors = ` argument (substituting an integer in) to use the [`gsynth` method](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2584200) that uses cross-validation to estimate the weights for multi-synthetic control. 

Try creating an augmented synthetic control model. How do your balance and estimates compare? 

```{r}
scm_gsyn <- multisynth(lnppexpend ~ cbr, State, year,
                        collective_bargaining_clean, n_leads = 10, 
                        fixedeff = T, n_factors = 2)

scm_gsyn_summ <- summary(scm_gsyn)
```

```{r}
scm_gsyn_summ$att %>%
  ggplot(aes(x = Time, y = Estimate, color = Level)) +
  geom_point() +
  geom_line() +
  geom_vline(xintercept = 0) +
  theme_fivethirtyeight() +
  theme(axis.title = element_text(),
        legend.position = 'None') +
  ggtitle('Augmented Synthetic Controls for State Collective Bargaining') +
  xlab('Time') +
  ylab('Expenditure on Pupil Estimate') +
  facet_wrap(~Level)
```

