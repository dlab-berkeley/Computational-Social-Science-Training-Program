{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dlab-berkeley/Computational-Social-Science-Training-Program/blob/master/Deep_Learning_and_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DrTKxZ1hCS5"
   },
   "source": [
    "# [Computational Social Science]\n",
    "## 5-5 Deep Learning and Tensorflow - Student Version\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHoISf71wzH2"
   },
   "source": [
    "This notebook will introduce you to the fundamentals of Tensorflow and explore techniques for deep learning with text data. Key concepts covered in this notebook include:\n",
    "\n",
    "1. Google Colab and GPUs\n",
    "2. Tensors and basic tensor operations\n",
    "3. Use tensorflow/keras to adapt and tune neural nets\n",
    "4. Existing resources to help analyze language data\n",
    "\n",
    "\n",
    "With these basic building blocks, you will be equipped to explore and implement deep learning algorithms for your own project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNqj6s9msZga"
   },
   "source": [
    "# Google Colab\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNovoJcyugcM"
   },
   "source": [
    "Objectives:\n",
    "\n",
    "- Set up a Google Colab notebook\n",
    "- Create, delete, run, and edit cells\n",
    "- Cover variable, notebook and package management\n",
    "\n",
    "15 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_zANHRDhplZ"
   },
   "source": [
    "## Introducing Google Colab\n",
    "\n",
    "\n",
    "Google Colab is a platform for cloud-based computation and coding. It is similar to a Jupyter Notebook, where individual cells of code are executed sequentially. It doesn't require local installation on your computer and can be shared and edited by multiple people at the same time. However the Colab notebook requires you to be connected to the internet, while jupyter notebooks can be run on your local machine. Google Colab notebooks are in the .ipynb format, and can be saved and opened either directly or via Google Drive. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqx972vJvdVv"
   },
   "source": [
    "##Basic Operations\n",
    "\n",
    "Google Colab has several features that help organize code and long notebooks. A few key concept to know to use this notebook effectively are:\n",
    "\n",
    "- Use the Insert tab in the upper bar, or press the +Code/+Text buttons in the top left of the window.\n",
    "\n",
    "- Text cells can be edited and formatting with the buttons at the top of the cell.\n",
    "\n",
    "- The buttons at the top right of the cell give you options to move, modify and delete the cell. \n",
    "\n",
    "- You can run code with shift+enter, or by clicking the top left of the box.\n",
    "\n",
    "- For more commands, use ctl+shift+p and select the desired command from the command palette\n",
    "\n",
    "An example code cell is below. Try executing and editing the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vhl6KsHCrLP-",
    "outputId": "cbce486b-44b1-4357-e996-b83be787a235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Google Colab\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to Google Colab\")\n",
    "x=12+78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pT9L_Th9sPrV"
   },
   "source": [
    "The buttons on the left panel help manage the notebook (search, table of contents, files). This is important for organizing your code and navigating long notebooks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VR9JpuTHcmVz"
   },
   "source": [
    "## Package Management\n",
    "\n",
    "Like Anaconda, Google Colab comes with many packages already available, and you can also install local packages using pip. Use the following lines of code in order to see which packages you have and which ones you need to install.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "#check which packages you have available (listed alphabetically). The version numbers are also avaliable which can be useful in determining issues with coding between computers.\n",
    "!pip list\n",
    "\n",
    "#install a new package\n",
    "!pip install numpy \n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<List of packages that we will use in this tutorial>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQ3GQZB7bo6M"
   },
   "source": [
    "In the following cell are the packages that you will need to complete this notebook. Run this line of code to make sure that everything is installed properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qey74xT0boGX"
   },
   "outputs": [],
   "source": [
    "#import packages for deep learning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pi2ikVCaoNct"
   },
   "source": [
    "## Customization\n",
    "\n",
    "Finally, there are several settings that you can customize if you so choose. These can be found under Tools -> Settings, where you can change the font size, background, and other aesthetic settings of the notebook to suit you. \n",
    "\n",
    "In addition, in Tools -> Keyboard shortcuts you can view and adapt shortcuts to your preferences as well.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHrAjcLHsYC3"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "Try out the following exercises to get comfortable with the new interface:\n",
    "\n",
    "1) Open the editor settings (Tools-> Settings->Editor) and select \"Show line numbers\". Now your cells will have line numbers next to them, which we can refer to when discussing code during this workshop.\n",
    "\n",
    "2) Make a new code cell below and save the product of 60 and 72 to a new variable. Then check the value of the variable in the variable tab to the left.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R2iLcnJj1-Ng"
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A58nCiMsshpn"
   },
   "source": [
    "# Introduction to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1AkXBaoujUc"
   },
   "source": [
    "Objectives:\n",
    "- Understand the benefits of GPUs\n",
    "- Set up GPU for Google Colab\n",
    "- Compare performance on tasks vs CPU\n",
    "\n",
    "\n",
    "10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp2UXU5HHTSM"
   },
   "source": [
    "As you've found in your previous models, some models take a significant amount of time to run. Models may also exceed the capacity of the local computer's processing power. This will either result in code that never finished running, or an error message indicating that the code has timed out without completing. \n",
    "\n",
    "To counteract this issue, TPU/GPU are parallel processing units that greatly speed up models. This can make some models that are otherwse impossible to train possible (Think minutes rather than hours)\n",
    "\n",
    "TPU is made specifically for tensorflow architecture, and speeds it up even more than GPUs.\n",
    "\n",
    "## GPU Access\n",
    "Oftentimes you need to pay for cloud services and access to GPUs, but one advantage of Colab is that it has free access to a certain amount of GPU/TPU units. This access is somewhat limited, but should be more than enough for what we are using it for today. We will discuss limitations and further options for long-term use in a later section of the workshop.\n",
    "\n",
    "\n",
    "Additional resource: https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=sXnDmXR7RDr2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "co9jxID0Utsj"
   },
   "source": [
    "The notebook will automatically choose which device (read: GPU vs CPU) to run the code on, but if you want to make sure that something is being run on a certain device, you can select a specific device as in the snippet below. \n",
    "\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "with tf.device(device_name):\n",
    "  #put task here\n",
    "  #return output\n",
    "```\n",
    "\n",
    "For now, we will trust the notebook's/ Tensorflow's allocation of computing power.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9xyYBVVRZlE"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "1) Run the following lines of code to test how fast your computer can do a task. Report the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uqgQQ2a1SBDj",
    "outputId": "46f5ae18-3f14-42e7-a0ad-c269fc4e9515"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2927398080000785\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "print(timeit.timeit('[x**2 for x in range(10)]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yx9qArhdR-31"
   },
   "source": [
    "2)  Change the settings to use GPU:  Edit --> Notebook Settings --> Hardware Accelerator --> GPU\n",
    ". Run the code below to make sure GPU is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ow0GhY3ANepm",
    "outputId": "3b96fcac-bffb-416c-ec14-4f11816e7227"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "#run this code to check that you have the GPU enabled\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6kl-9FvVbBD"
   },
   "source": [
    "3) Re-run the same timing task and report your results. How much of a difference is there in timing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9ADVC5SVabz",
    "outputId": "4ba3f95e-3047-45f3-ca52-541a020f1c6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.587992389000192\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "print(timeit.timeit('[x**2 for x in range(10)]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GvtUiHVV6IO"
   },
   "source": [
    "As we run more complex tasks, the efficiency of GPUs becomes more and more of a difference. If you are curious, you can compare the timing of the tasks in this notebook with GPU/TPU/CPU and note the difference. Even though in this notebook we are working with fairly small dataset and task, these differences will be important at larger scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_hXKH0wu5XL"
   },
   "source": [
    "# Manpulating Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L9b-y83u6bR"
   },
   "source": [
    "Objectives:\n",
    "- Understand the tensor data type\n",
    "- Index, reshape, and slice tensors\n",
    "\n",
    "20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGfJ5yBMaDa0"
   },
   "source": [
    "In the Neural Networks section of the course, you were introduced to Multilayer Perceptrons (MLP) as a basic building block of neural networks. You were also introduced to using Keras to build a neural network for digit recognition. Neural networks are powerful deep learning tools that can learn complex relationships in data. In this section we will further explore the powerful [Tensorflow](https://www.tensorflow.org/) framework for neural networks.\n",
    "\n",
    "First, we will cover [tensors](https://www.tensorflow.org/guide/tensor), which are an essential concept for interacting with deep learning models. Tensors are the key data structure in Tensorflow and are simlar to numpy arrays but are compatible with GPUs. Tensors have one or more dimensions, are rectangular, and are immutable. Every entry in a tensor must have the same datatype (usually float).\n",
    "\n",
    "A 3-dimensional tensor can be visually represented in a few different ways. It can be represented as a mxnxp dimensional block:\n",
    "\n",
    "\n",
    "![3-axis_block.png](https://drive.google.com//uc?id=1ZQIeFD5zm-Nnh28bfgooXnb0AqlaWNzB)\n",
    "\n",
    "\n",
    "Or the block can be flattened out to three mxn dimensional arrays:\n",
    "\n",
    "![3-axis_numpy.png](https://drive.google.com//uc?id=1TyHhSZ66fJcFYGGkHrnmI3ZlNZ7-5RJW)\n",
    "\n",
    "\n",
    "The *shape* of this tensor is 5x3x2 and the *size* is 30, since there are 30 total units in the tensor. Although they are harder to visualize, tensors can have many dimensions.\n",
    "\n",
    "\n",
    "In Tensorflow, tracking the dimensions, shapes, sizes of the tensors is an essential skill for working with this code. This is key in allowing us to take datasets and adapt them to existing models. We will work through an exercise demonstrating these ideas in the next sections.\n",
    "\n",
    "Images from:  https://www.tensorflow.org/guide/tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHzdlBc3dtqk"
   },
   "source": [
    "## Tensor Operations\n",
    "\n",
    "Just like manipulating dataframes or arrays, manipulating tensors is an important skill. There are some basic tensor operations that it is useful to be aware of for manipulating tensors. We will focus on the key operations for deep learning: indexing and reshaping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6f-T3hs4GR9"
   },
   "source": [
    "### Creating a tensor from an array\n",
    "\n",
    "Tensors are similar to numpy array and tensorflow will automatically convert an array to a tensor when using tensorflow operations. Similarly, .numpy() can convert a tensor to an array.\n",
    "\n",
    "\n",
    "More commonly, you will most likely be using methods that process data and output a tensor that you can then work with, and most conversions to tensors will be handled automatically within those methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3GUXRFkAe59o",
    "outputId": "a32e36ae-d1c3-4bc4-88cf-12181dc10f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original data type is: <class 'numpy.ndarray'>\n",
      "The new data type is: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(\n",
      "[[[ 0  1  2  3  4]\n",
      "  [ 5  6  7  8  9]]\n",
      "\n",
      " [[10 11 12 13 14]\n",
      "  [15 16 17 18 19]]\n",
      "\n",
      " [[20 21 22 23 24]\n",
      "  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#make a sample tensor\n",
    "data=np.array([[[0,1,2,3,4],[5,6,7,8,9]],\n",
    "      [[10,11,12,13,14],[15,16,17,18,19]],\n",
    "      [[20,21,22,23,24],[25,26,27,28,29]]])\n",
    "print('The original data type is:',type(data))\n",
    "sample_tensor=tf.concat(data,2)\n",
    "print('The new data type is:',type(sample_tensor))\n",
    "print(sample_tensor)\n",
    "\n",
    "#to convert from tensor to array: sample_tensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdB9XbUQd45i"
   },
   "source": [
    "What is the shape of the tensor? What is the total size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rhd9K7eQU8AE"
   },
   "source": [
    "###Indexing\n",
    "\n",
    "[Indexing](https://www.tensorflow.org/guide/tensor) allows you to select subsections of the tensor, similar to subsetting an array or DataFrame. You can select a single number, or range of numbers in the tensor by specifying the position of the number in each dimension. It's useful to build an intuition for indexing, since it is a common method in Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2gLiSuEke8RK",
    "outputId": "d685b8cd-e6c1-4fa1-da61-5c3db01eb456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#get a single number\n",
    "sample_tensor[0,0,0]\n",
    "print(sample_tensor[0,0,0].numpy()) #.numpy() converts it to an array to print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Crc0H-BJinpT",
    "outputId": "a64ab204-03fa-423f-f5a2-dd1f0fb499e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8]\n",
      " [18]]\n"
     ]
    }
   ],
   "source": [
    "#get a range of values\n",
    "print(sample_tensor[0:2,1:2,3].numpy()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Ix9PLZxiolK",
    "outputId": "60f85fba-a29e-4bca-e422-abdd64bae3bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3 4]\n",
      " [5 6 7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "#take all items in a dimension:\n",
    "print(sample_tensor[0,:,:].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXpczotQircG"
   },
   "source": [
    "Refer to the output of the following code. What is the output?\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "sample_tensor[:,0,:].numpy()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c57tLhXqjDvm",
    "outputId": "c7150b92-66ec-4cae-80a6-6e893eefe53c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0  1  2  3  4]\n",
      "  [ 5  6  7  8  9]]\n",
      "\n",
      " [[10 11 12 13 14]\n",
      "  [15 16 17 18 19]]\n",
      "\n",
      " [[20 21 22 23 24]\n",
      "  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(sample_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hwbhy6_NjibE"
   },
   "source": [
    "Getting used to translating between the printed tensor dimensions and the output is an essential skill for selecting relevant information from a tensor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAnsigU4WYhc"
   },
   "source": [
    "### Reshaping\n",
    "[Reshaping](https://www.tensorflow.org/api_docs/python/tf/reshape) is another key operation for manipulating tensors. Reshaping tensors, like arrays, can include switching, increasing, and decreasing dimensions. For example, you can change the three dimensional tensor into one dimension or two dimensions. The best way to understand the arguments for reshaping is to practice and look at examples. A series of sample reshapings are given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MAqHTlmIhlvL",
    "outputId": "36947a71-0af3-4124-db28-b8ae1fd151e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original tensor:\n",
      "tf.Tensor(\n",
      "[[[ 0  1  2  3  4]\n",
      "  [ 5  6  7  8  9]]\n",
      "\n",
      " [[10 11 12 13 14]\n",
      "  [15 16 17 18 19]]\n",
      "\n",
      " [[20 21 22 23 24]\n",
      "  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int64)\n",
      "\n",
      "Shaped tensor:\n",
      "tf.Tensor(\n",
      "[[[ 0  1  2  3  4]\n",
      "  [ 5  6  7  8  9]\n",
      "  [10 11 12 13 14]]\n",
      "\n",
      " [[15 16 17 18 19]\n",
      "  [20 21 22 23 24]\n",
      "  [25 26 27 28 29]]], shape=(2, 3, 5), dtype=int64)\n",
      "\n",
      "Compare the size of the two tensors:\n",
      "tf.Tensor(30, shape=(), dtype=int32) tf.Tensor(30, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#reshaping\n",
    "\n",
    "print(\"\\nOriginal tensor:\")\n",
    "print(sample_tensor)\n",
    "print(\"\\nShaped tensor:\")\n",
    "print(tf.reshape(sample_tensor,(2,3,5))) #switch first and second dimensions\n",
    "\n",
    "print(\"\\nCompare the size of the two tensors:\")\n",
    "print(tf.size(sample_tensor),tf.size(tf.reshape(sample_tensor,(2,3,5))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-D-v_xp5zFXD"
   },
   "source": [
    "You can also change the 3-dimensional tensor to a 1-dimensional tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kAhOVc1Hfw67",
    "outputId": "7b3006c6-b609-4019-8652-a19abc6e2ac2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29], shape=(30,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#flatten tensor\n",
    "print(tf.reshape(sample_tensor,(30)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fv0mggCiz7aa"
   },
   "source": [
    "Or you can change it to two dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zr6SCYJQf4uY",
    "outputId": "743495da-570a-4cdf-e76a-7008cddb141e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      " [15 16 17 18 19 20 21 22 23 24 25 26 27 28 29]], shape=(2, 15), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[ 0  1  2  3  4  5  6  7  8  9]\n",
      " [10 11 12 13 14 15 16 17 18 19]\n",
      " [20 21 22 23 24 25 26 27 28 29]], shape=(3, 10), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#reduce to two dimensions\n",
    "print(tf.reshape(sample_tensor,(2,15)))\n",
    "print(tf.reshape(sample_tensor,(3,10)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88mynebvDWXg"
   },
   "source": [
    "However, the size of the reshaped tensor must match the size of the original tensor. For example, the following code will not run without an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "j672TMU9DU6u",
    "outputId": "f29ad7dc-4a3b-478d-c485-845629d8cc14"
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-853e99059d0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#this code will run with an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 30 values, but the requested shape has 20 [Op:Reshape]"
     ]
    }
   ],
   "source": [
    "#this code will run with an error\n",
    "\n",
    "print(tf.reshape(sample_tensor,(2,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyI1qmIW0A5n"
   },
   "source": [
    "With reshaping, it is essential to be confident that the type of reshaping is appropriate for the task. If the sizes don't match, the code will return an error. Even harder to track down is reshaping where the sizes match, but the dimensions do not align with intended reshaping. This is important because with reshaping you can get bugs that do not throw errors but result in problems in the final model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0TRi6xkkNmS"
   },
   "source": [
    "Let's do some practice with indexing and reshaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f2-hzVtJUq6V",
    "outputId": "649c7ec3-a617-4052-b75e-94704688d3d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 2  4]\n",
      "  [ 6  8]]\n",
      "\n",
      " [[10 12]\n",
      "  [14 16]]\n",
      "\n",
      " [[ 1  3]\n",
      "  [ 5  7]]\n",
      "\n",
      " [[ 9 11]\n",
      "  [13 15]]]\n",
      "tf.Tensor([4 2 2], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "data=[[[2,4],[6,8]],\n",
    "      [[10,12],[14,16]],\n",
    "      [[1,3],[5,7]],\n",
    "      [[9,11],[13,15]],\n",
    "      ]\n",
    "sample_tensor_2=tf.stack(data)  \n",
    "print(sample_tensor_2.numpy())\n",
    "print(tf.shape(sample_tensor_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jXSm8JapiZzz"
   },
   "source": [
    "## Challenge\n",
    "\n",
    "Use sample_tensor_2 for the input and complete the following challenges below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hzz_odBH2ZzX"
   },
   "source": [
    "1) What is the total size of the dataset?\n",
    "\n",
    "2) How many samples are in the dataset? How many entries are there per sample?\n",
    "\n",
    "3) What do you predict the following code will do? What is the shape of the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D1e0Tz5z43ha",
    "outputId": "ade949b3-de1e-408b-c537-c7571e4976a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=int32, numpy=\n",
       "array([[ 4,  8],\n",
       "       [12, 16],\n",
       "       [ 3,  7],\n",
       "       [11, 15]], dtype=int32)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tensor_2[:,:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZwpIEHed_9C"
   },
   "source": [
    "## Reshaping a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtToiL2TUrj4"
   },
   "source": [
    "\n",
    "Objectves:\n",
    "  - Adapt a model to an existing architecture\n",
    "\n",
    "10 minutes\n",
    "\n",
    "An important skill is being able to reshape a dataset into a shape approprate for a given model. For example, tensor from the challenges above was three dimensions, with two dimensions of features per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tPx074SSeF_J",
    "outputId": "add5ad21-8b2f-455e-ea12-af1c7ea5d665"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 2  4]\n",
      "  [ 6  8]]\n",
      "\n",
      " [[10 12]\n",
      "  [14 16]]\n",
      "\n",
      " [[ 1  3]\n",
      "  [ 5  7]]\n",
      "\n",
      " [[ 9 11]\n",
      "  [13 15]]], shape=(4, 2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(sample_tensor_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnTzpx1GVHwM"
   },
   "source": [
    "However, many common neural networks would expect 1-dimensional data as an input, so we can use reshaping to get 1-dimensional data. What shape would we expect the input tensor to be to fit the model? Hint: it still needs to be the same size as the original tensor. \n",
    "\n",
    "Once we have an expectation of what to do, then we can translate it into code. Which of the following options do you thnk would work? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4IlbghWAVIG2",
    "outputId": "b123310c-a684-4361-d8b5-d95574d2fc01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=int32, numpy=\n",
       "array([[ 2,  4,  6,  8],\n",
       "       [10, 12, 14, 16],\n",
       "       [ 1,  3,  5,  7],\n",
       "       [ 9, 11, 13, 15]], dtype=int32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reshape(sample_tensor_2,(4,4,1))\n",
    "\n",
    "tf.reshape(sample_tensor_2,(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pT58gaj2cqPk"
   },
   "source": [
    "Since we want the features in each sample to be one-dimensional, we would go with option two. Finally, we would check the output tensor to make sure it matches our expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxPwd6pgeKhq",
    "outputId": "f96f8ba7-4e14-4128-9ea7-2e4907fd22a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2  4  6  8]\n",
      " [10 12 14 16]\n",
      " [ 1  3  5  7]\n",
      " [ 9 11 13 15]], shape=(4, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reshape(sample_tensor_2,(4,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgkQ8x8JlGKH"
   },
   "source": [
    "This process is very common in taking a raw dataset and adapting it to a neural network architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TR78IdDij7M"
   },
   "source": [
    "# Deep Learning\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6twFHgyZSeiv"
   },
   "source": [
    "Objectives: \n",
    "- Code and optimize a neural network\n",
    "- Adapt a network to new data\n",
    "\n",
    "20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRq8wjQaqiWW"
   },
   "source": [
    "In previous sections of this course, you have covered neural networks and deep learning for classifying the MNIST dataset. The task was classifying handwritten digits 0-9 based on images. In this section, we will revisit deep learning in Python with text data. \n",
    "\n",
    "We will start with the classificaton problem (student loan vs checking/savings account) from the NLP section of the course, where customer complaint data was used classify what type of account the complaint was related to. We will use the same embeddings we trained for the final logistic regression problem in that section of the course.\n",
    "\n",
    "First, we will load in the data and split it into training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMyrg9cv0pDS"
   },
   "outputs": [],
   "source": [
    "word2vec_features_df=pd.read_csv('https://github.com/dlab-berkeley/Computational-Social-Science-Training-Program/raw/master/data/embeddings.csv')\n",
    "y=pd.read_csv('https://github.com/dlab-berkeley/Computational-Social-Science-Training-Program/raw/master/data/y.csv')\n",
    "y_vals=y['Product_binary'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(word2vec_features_df, \n",
    "                                                    y_vals, \n",
    "                                                    train_size = .80, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhyFnEKUh_uz"
   },
   "source": [
    "Next we define the model. In Keras, each layer of the model has to be individually specified. This allows significant control over the model, including different parameters for each level.\n",
    "\n",
    "This model has a dense layer with 128 neurons in each, and a dropout layer where 20% of the connections are dropped out for each layer. The final output layer uses a sigmoid activation function to create a final binary output (0 or 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colGv5dcuzwP"
   },
   "outputs": [],
   "source": [
    "def NN_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    # A fully connected layer with 128 neurons\n",
    "    model.add(Dense(128, input_dim=301,activation='relu'))\n",
    "\n",
    "    # A dropout layer that randomly excludes 20% of neurons in the layer \n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # An output layer with binary classification\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model with crossentropy\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8wH5KqiKiQQl"
   },
   "source": [
    "Finally, we fit and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9AXKsZQwVsR",
    "outputId": "f0c4eaae-7fa3-432b-8be7-cdfc1c2473c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Error: 21.50%\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 128)               38656     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38,785\n",
      "Trainable params: 38,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = NN_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=0)\n",
    "\n",
    "# Evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"NN Error: %.2f%%\" % (100-scores[1]*100))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIN6sK82FQxp"
   },
   "source": [
    "This is a simple neural network with a couple of densely connected layers and a couple of dropout layers. When working with neural nets, it's often a good idea to start with a simple net to make sure the basics of the code work, then gradually create more complicated architectures once the code runs smoothly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGLS9pEUibTB"
   },
   "source": [
    "Now, let's use our tensor knowledge to adapt this architecture to another set of data. First, let's load in the MNIST digits dataset (in practice, we would likely be using a dataset more similar to the one in the original model). The MNIST dataset is three dimensions (n_samplesx28x28), so we need to flatten the data for now to create a two dimensional tensor  n_samplesx784 to fit with the neural net we are working on. Note: instead of two classes, the MNIST dataset uses 10 classes (one for each digit 0-9). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZTIeDTnBn8o",
    "outputId": "675f8530-f021-4b9a-ee93-d8dbb6d30e28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "11501568/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# reshape to [samples][width][height][pixels]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28*28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28*28)\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bP926VQjIJ6"
   },
   "source": [
    "Here is the same code from the NN model above. What do you need to change in order to run the same model on the new data? Note which parameters and values you need to change. How does this relate to the differences in the data? Let's edit the code to work with the new data shape and execute it.\n",
    "\n",
    "Hint: use tf.shape() to see the compare the shapes of the MNIST and original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DtWkyXLfNbk"
   },
   "source": [
    "These are the lines of code we need to change to make this model work with new data: \n",
    "\n",
    "\n",
    "In line 6: \n",
    "```\n",
    "model.add(Dense(128, input_dim=784,activation='relu')) #change input dim\n",
    "```\n",
    "The embeddings dataset had 301 features, or columns, the new MNIST dataset has 784, so we need to make sure to match the numbers in model architecture. \n",
    "\n",
    "In line 12: \n",
    "\n",
    "```\n",
    "    model.add(Dense(10, activation='softmax')) #change dimension to number of categories\n",
    "```\n",
    "The final layer needs to have 10 categories, rather than two, since there are more classes in the MNIST dataset. In addition, the activation function needs to be changed to softmax.\n",
    "\n",
    "In lne 15:\n",
    "\n",
    "```\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "```\n",
    "\n",
    "Again, because of the number of classes, the loss function used must be categorical cross entropy rather than binary cross entropy. \n",
    "\n",
    "Here is the updated model:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BsgLoMccgRtr",
    "outputId": "1ec4d26e-d308-4b7d-a8de-4be76e12ae88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "Epoch 1/10\n",
      "300/300 - 3s - loss: 5.1993 - accuracy: 0.6690 - val_loss: 0.7474 - val_accuracy: 0.8304 - 3s/epoch - 10ms/step\n",
      "Epoch 2/10\n",
      "300/300 - 2s - loss: 0.8942 - accuracy: 0.7889 - val_loss: 0.5607 - val_accuracy: 0.8722 - 2s/epoch - 7ms/step\n",
      "Epoch 3/10\n",
      "300/300 - 2s - loss: 0.6801 - accuracy: 0.8358 - val_loss: 0.4515 - val_accuracy: 0.9023 - 2s/epoch - 7ms/step\n",
      "Epoch 4/10\n",
      "300/300 - 2s - loss: 0.5592 - accuracy: 0.8656 - val_loss: 0.3760 - val_accuracy: 0.9144 - 2s/epoch - 6ms/step\n",
      "Epoch 5/10\n",
      "300/300 - 2s - loss: 0.4838 - accuracy: 0.8822 - val_loss: 0.3555 - val_accuracy: 0.9227 - 2s/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "300/300 - 1s - loss: 0.4250 - accuracy: 0.8959 - val_loss: 0.3256 - val_accuracy: 0.9285 - 943ms/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "300/300 - 1s - loss: 0.3735 - accuracy: 0.9068 - val_loss: 0.2784 - val_accuracy: 0.9346 - 967ms/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "300/300 - 1s - loss: 0.3363 - accuracy: 0.9152 - val_loss: 0.2726 - val_accuracy: 0.9374 - 933ms/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "300/300 - 1s - loss: 0.3114 - accuracy: 0.9216 - val_loss: 0.2449 - val_accuracy: 0.9443 - 923ms/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "300/300 - 1s - loss: 0.2833 - accuracy: 0.9273 - val_loss: 0.2343 - val_accuracy: 0.9447 - 911ms/epoch - 3ms/step\n",
      "NN Error: 5.53%\n"
     ]
    }
   ],
   "source": [
    "def diff_CNN_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(128, input_dim=784,activation='relu')) #change input dim\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(.2))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax')) #change dimension to number of categories and activation function\n",
    "    \n",
    "    #change to categorical crossentropy\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = diff_CNN_model()\n",
    "# Fit the model\n",
    "print(X_train.shape)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "\n",
    "# Evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"NN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23KhRlCwGLSi"
   },
   "source": [
    "# Optimizing Neural Nets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOPuwHOBSt2l"
   },
   "source": [
    "Objectives:\n",
    "- Explore strategies to optimize a neural net\n",
    "- Implement an optimizer with custom settings\n",
    "- Grid search parameters\n",
    "\n",
    "20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZjXK0-OGT5C"
   },
   "source": [
    "Optimizing neural nets is a key point of using these powerful models effectively, as with any ML models. However, neural nets have many parameters that can be tuned and are a challenge for traditional optmization methods such as grid search.\n",
    "\n",
    "In the previous challenge, we experimented with improving the accuracy of the model. The following strategies can help guide the optmization process for fine-tuning algorithms.\n",
    "\n",
    "1. Feature engineering (refer to Natural Language Processing Notebook)\n",
    "\n",
    "2. Try a smaller network (minimize redundancy) or a larger network (capture more complex relationships)\n",
    "\n",
    "3. Change learning rate\n",
    "4. Use appropriate architecture for the data/task\n",
    "\n",
    "5. Test parameters\n",
    "\n",
    "6. Decrease batch size \n",
    "\n",
    "Depending on the task, data, and neural network used, there may be a significant amount of tuning necessary in order to achieve an optimal result. This is one reason why leveraging existing models that are already optimized can give a huge advantage for language tasks. \n",
    "\n",
    "Further reference this article: https://towardsdatascience.com/optimizing-neural-networks-where-to-start-5a2ed38c8345 \n",
    "\n",
    "\n",
    "For this notebook we will start with changing the learning rate. \n",
    "\n",
    "In previous examples, we passed the optimizer to the compile funciton \n",
    "```\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "```\n",
    "Which uses the default parameters for the function. Now that we are customizing the parameters, we want to use the actual optimizer function, and then pass that optimizer into the .compile() function.\n",
    "\n",
    "```\n",
    "model.compile(....,opt=keras.optimizers.Adam())\n",
    "```\n",
    "\n",
    "Here is the documentation for that function: https://keras.io/api/optimizers/adam/\n",
    "\n",
    "What is the default parameter for learning rate? What are some of the other parameters for the Adam optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElzhbM-eOPDe"
   },
   "source": [
    "##Challenge\n",
    "\n",
    "Test the following learning rates: [.0001,.001,.01,.1]. Which one performs the best? Which one performs the worst?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DZBPIH2OObp"
   },
   "outputs": [],
   "source": [
    "#load in data to use for this test\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#cnn classification for neural nets\n",
    "word2vec_features_df=pd.read_csv('https://github.com/dlab-berkeley/Computational-Social-Science-Training-Program/raw/master/data/embeddings.csv')\n",
    "\n",
    "y=pd.read_csv('https://github.com/dlab-berkeley/Computational-Social-Science-Training-Program/raw/master/data/y.csv')\n",
    "y_vals=y['Product_binary'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(word2vec_features_df, \n",
    "                                                    y_vals, \n",
    "                                                    train_size = .80, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state = 10)\n",
    "#print(word2vec_features_df.shape)\n",
    "#print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Trjy8DMSQCGj",
    "outputId": "c510a4f3-ce75-4ee3-c5a1-f20f8518745e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 301)\n",
      "Epoch 1/10\n",
      "4/4 - 1s - loss: 137.0990 - accuracy: 0.6562 - val_loss: 15.7502 - val_accuracy: 0.2150 - 862ms/epoch - 215ms/step\n",
      "Epoch 2/10\n",
      "4/4 - 0s - loss: 5.8819 - accuracy: 0.3587 - val_loss: 3.8590 - val_accuracy: 0.7850 - 39ms/epoch - 10ms/step\n",
      "Epoch 3/10\n",
      "4/4 - 0s - loss: 1.3224 - accuracy: 0.7837 - val_loss: 1.0794 - val_accuracy: 0.7850 - 34ms/epoch - 8ms/step\n",
      "Epoch 4/10\n",
      "4/4 - 0s - loss: 2.0045 - accuracy: 0.7825 - val_loss: 0.5748 - val_accuracy: 0.7850 - 35ms/epoch - 9ms/step\n",
      "Epoch 5/10\n",
      "4/4 - 0s - loss: 0.5777 - accuracy: 0.7862 - val_loss: 0.5711 - val_accuracy: 0.7850 - 52ms/epoch - 13ms/step\n",
      "Epoch 6/10\n",
      "4/4 - 0s - loss: 0.5623 - accuracy: 0.7862 - val_loss: 0.5472 - val_accuracy: 0.7850 - 34ms/epoch - 9ms/step\n",
      "Epoch 7/10\n",
      "4/4 - 0s - loss: 0.5403 - accuracy: 0.7862 - val_loss: 0.5328 - val_accuracy: 0.7850 - 38ms/epoch - 10ms/step\n",
      "Epoch 8/10\n",
      "4/4 - 0s - loss: 0.5319 - accuracy: 0.7862 - val_loss: 0.5236 - val_accuracy: 0.7850 - 35ms/epoch - 9ms/step\n",
      "Epoch 9/10\n",
      "4/4 - 0s - loss: 0.5214 - accuracy: 0.7862 - val_loss: 0.5208 - val_accuracy: 0.7850 - 35ms/epoch - 9ms/step\n",
      "Epoch 10/10\n",
      "4/4 - 0s - loss: 0.5205 - accuracy: 0.7862 - val_loss: 0.5207 - val_accuracy: 0.7850 - 34ms/epoch - 8ms/step\n",
      "CNN Error: 21.50%\n"
     ]
    }
   ],
   "source": [
    "model = NN_model()\n",
    "# Fit the model\n",
    "print(X_train.shape)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "\n",
    "# Evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AXRqpHOqKxy"
   },
   "source": [
    "# Challenge: Optimizing a Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZj0IiVYjYtY"
   },
   "source": [
    "\n",
    "The logit model from the challenge question in NLP section used to classify the customer complaint data had an accuracy of 78.5%. What is the accuracy of the first neural network model on the same data? Hint: (read the output) Try changing the model to improve accuracy. What configuration gave you the best results? Try changing the parameters of the existing layers, or adding more layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSEW7T0Vpacv"
   },
   "outputs": [],
   "source": [
    "word2vec_features_df=pd.read_csv('https://github.com/dlab-berkeley/Computational-Social-Science-Training-Program/raw/master/data/embeddings.csv')\n",
    "y=pd.read_csv('https://github.com/dlab-berkeley/Computational-Social-Science-Training-Program/raw/master/data/y.csv')\n",
    "y_vals=y['Product_binary'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(word2vec_features_df, \n",
    "                                                    y_vals, \n",
    "                                                    train_size = .80, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpMjqhdt3HKh",
    "outputId": "22d37ed5-0509-47ae-e1c7-c63cae19b9b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN Error: 21.50%\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_7 (Dense)             (None, 128)               38656     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55,297\n",
      "Trainable params: 55,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#original model\n",
    "\n",
    "def NN_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    # A fully connected layer with 128 neurons\n",
    "    model.add(Dense(128, input_dim=301,activation='relu'))\n",
    "\n",
    "    # A dropout layer that randomly excludes 20% of neurons in the layer \n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # An output layer with binary classification\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model with crossentropy\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "    model = NN_model()\n",
    "    \n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=0)\n",
    "\n",
    "# Evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"NN Error: %.2f%%\" % (100-scores[1]*100))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZEJquJOwESl"
   },
   "source": [
    "In practice we often take advantage of existing code and architecture to help accomplish deep learning tasks. This can range from taking a neural network architecture and adapting it to new data (as in our exercise above) to using other packages with pre-trained models. In the next section we will explore one such package called Huggingface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5Ovl5RF2DjS"
   },
   "source": [
    "# Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRXZt7DhS7mR"
   },
   "source": [
    "Objectives:\n",
    "- Explore tasks and data available in Huggingface transformers\n",
    "- Choose an appropriate language task\n",
    "- Implement a transformer on local data\n",
    "\n",
    "20 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glKdUE_CEw7h"
   },
   "source": [
    "In reality, these models  require significant data and computational power, which can exceed the resources available to the analyst. We can circumvent this problem by using pre-trained models. Like a pre-trained embedding model, pre-trained models are trained on a large dataset. While this may not perfectly align with the data or task you have, it can help create a more robust system that can be fine-tuned to your data and goals.\n",
    "\n",
    "[Huggingface](https://huggingface.co/models) is a set of pretrained models from a variety of datasets and sources with an easy-to-use interface. In this section, we will explore the use of the Huggingface library to streamline language task processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pE52xADz3Py-",
    "outputId": "f90ced77-a436-4b50-893c-c7a24ad4bd00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
      "\u001b[K     || 3.5 MB 5.2 MB/s \n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.10.1\n",
      "  Downloading tokenizers-0.11.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
      "\u001b[K     || 6.8 MB 33.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.0)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     || 596 kB 45.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
      "\u001b[K     || 895 kB 46.3 MB/s \n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "\u001b[K     || 67 kB 5.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.5 transformers-4.16.2\n"
     ]
    }
   ],
   "source": [
    "#install the transformers library\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WygeyX2MQo8T"
   },
   "source": [
    "The simplest strategy is to use the pipeline method, where you select the task and the pre-trained model (there are multiple models available for many of the tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183,
     "referenced_widgets": [
      "86034a47f8514079912c35155d146369",
      "5caaf9bc55d340a3b89976d4aeacb332",
      "19f163d889894c0f83fd41f349dc9ada",
      "36fb982530874258b232df0fe36033f4",
      "58a368b25b4b406ea7b6bb91c329c6c6",
      "ebf98e51ca7144f4a7b34949464afb64",
      "ff8cf5b835444cdca63fba8c0d6acc49",
      "a8f010ab78494b409ca4b4328529964b",
      "4bacf0649f254a0d989a162713e8eb52",
      "38589bd193514f709dd5ce7d2e5bedcd",
      "a501a28500024c19892528550569f133",
      "7994fbb978ff455a8d0de80b794af494",
      "1c77193b7ef34de1a258ea3865ef8d00",
      "707caeef65034f9aae75b4299122d3a4",
      "df3a99de4f434c3285b45a36c76d3490",
      "7f87188f6c0e47ca8e1b50b0add4f040",
      "26ff32c8075c4bfbbd599f9684a7cdea",
      "8f5505a6ec444f93b269c94cfd2043a7",
      "c40ffebf7195409e9408ee6adfada1e5",
      "54b4b9c017734ed48eb5e58ed9a01ee0",
      "f47b513d4d204815916385a3f920b97c",
      "31b7cb0179b44275923e1ab8709c7383",
      "6832ca1251124d82a08b8877ed5734ad",
      "c09a653d569648b3884a975eafc2daf9",
      "c05dce23107d4037a27dbd341edea56c",
      "dae54fa11bfb4ada8c6b403642022574",
      "c0913adaad46457fb9c75a983429e8db",
      "c1f08dbd4ab7467e9b41ff463d8af522",
      "4b675ac9211d464a8324603501ea571d",
      "a70a77484d564944bc8f3ff4f1a8d95b",
      "39b737c5a9a147c7b7edf620875926b3",
      "c746a04747a842718b63923829c6c159",
      "55b479d2a81c4a15b2f582395d55c986",
      "514e37a2cc81490b82ef4e2873927db6",
      "faed30b0652748e0950d163036860c05",
      "79fa7eca06fc4bc795eeda6f0c8b71ac",
      "74533b2febb645218408210aa9c4c12a",
      "2121c86cdd3d45ba8a8441e9409dd478",
      "23e32ddfec514ae5a5f3fa0443adf6ab",
      "c485b3fc092b45d2a0fce01bc6b2455c",
      "2ca131b4aace4bb3acbf17ed6be3ac77",
      "fe4e711d433740d0a97193210f171286",
      "1e676cd9ddba471bbe49250834a7345a",
      "e82e3db592ee48299f2c2da59f7508cd"
     ]
    },
    "id": "V9pq4nvXSnMh",
    "outputId": "e95d1167-f00d-4f25-e348-249ddfdc4235"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86034a47f8514079912c35155d146369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7994fbb978ff455a8d0de80b794af494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6832ca1251124d82a08b8877ed5734ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514e37a2cc81490b82ef4e2873927db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdXQGdLgQ9sa"
   },
   "source": [
    "The key to using these models, since the preprocessing is built in, is understanding the format of the data necessary for the model. This model takes the raw text as input rather than the word embeddings, so let's reload our data appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iahhf3AEUGBN",
    "outputId": "2bed42db-55d1-488d-de14-56ca082d78e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviewed my credit report in XX/XX/XXXX and noticed a lot of errors, inconsistent, and incorrect information. Sent a letter to Equifax on XX/XX/XXXX via mail asking them for an investigation and to verify all the dates and amounts were correct and fix the incorrect reporting on my credit. They did not respond at all so I sent another letter on XX/XX/XXXX via mail, again asking for an investigation and proof. They still didnt respond to that letter so I sent a third letter on XX/XX/XXXX certified mail so I have proof that they signed for my letter.\n",
      "\n",
      "Last week I received two letters from Equifax dated XX/XX/XXXX on the same day. The said that they could not locate my credit file and needed me to send proof of identification and address. With all three letters I sent a copy of my Arizona drivers license and my XXXX direct deposit sub as my proof of address. The second letter said that they received my request to be removed from the promotions list and that it was added to my credit file. How did they find my file to add the restriction if they couldnt find my credit file in regards to investigation purposes?\n"
     ]
    }
   ],
   "source": [
    "cfpb=pd.read_csv('https://raw.githubusercontent.com/dlab-berkeley/Computational-Social-Science-Training-Program/master/data/CFPB%202020%20Complaints.csv')\n",
    "complaints=cfpb['Consumer complaint narrative']\n",
    "complaints=complaints[~complaints.isna()]\n",
    "classifier(complaints.values[0])\n",
    "print(complaints.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_rQGYoPRNon"
   },
   "source": [
    "Then use the pipeline on the example data, and look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Thjw7tTaRSgH",
    "outputId": "92338531-acc0-4bae-e72f-9722761b9b37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviewed my credit report in XX/XX/XXXX and noticed a lot of errors, inconsistent, and incorrect information. Sent a letter to Equifax on XX/XX/XXXX via mail asking them for an investigation and to verify all the dates and amounts were correct and fix the incorrect reporting on my credit. They did not respond at all so I sent another letter on XX/XX/XXXX via mail, again asking for an investigation and proof. They still didnt respond to that letter so I sent a third letter on XX/XX/XXXX certified mail so I have proof that they signed for my letter.\n",
      "\n",
      "Last week I received two letters from Equifax dated XX/XX/XXXX on the same day. The said that they could not locate my credit file and needed me to send proof of identification and address. With all three letters I sent a copy of my Arizona drivers license and my XXXX direct deposit sub as my proof of address. The second letter said that they received my request to be removed from the promotions list and that it was added to my credit file. How did they find my file to add the restriction if they couldnt find my credit file in regards to investigation purposes?\n",
      "[{'label': 'NEGATIVE', 'score': 0.9993847608566284}]\n",
      "TransUnion has not properly investigated the items below. I am requesting they be removed from my report pending a thorough review. \n",
      "\n",
      "ACCOUNTS XXXX XXXX XXXX XXXX not mine XXXX XXXX XXXX XX/XX/2018 unauthorized XXXX XXXX XX/XX/2018 unauthorized XXXX  XXXX XX/XX/2018 unauthorized XXXX XXXX XXXX XX/XX/2018 unauthorized XXXX  XXXX I XX/XX/2018 unauthorized XXXX XXXX XXXX XX/XX/2018 unauthorized XXXX XXXX XXXX XX/XX/2018 unauthorized XXXX XXXX XXXX XXXX not mine\n",
      "[{'label': 'NEGATIVE', 'score': 0.9992923736572266}]\n",
      "XX/XX/2020 someone tried to steal my identity by opening up a credit card with XXXX  XXXX XXXX and XXXX XXXX XXXX XXXX XXXX and XXXX XXXXXXXX  XXXX ran a hard inquiry XXXX and XXXXXXXX XXXX XXXX  has since been removed from my credit report. Transunion and XXXX  are refusing to remove the XXXX XXXX hard inquiry. I have a letter from XXXX XXXX stating that Transunion and XXXX are to remove that hard inquiry because it consisted of fraud. Thank you. \n",
      "XXXX XXXX.\n",
      "[{'label': 'NEGATIVE', 'score': 0.9986603260040283}]\n",
      "I paid the debt on XX/XX/XXXX. I disputed account on credit report on XX/XX/XXXX. Dispute report was finalized on XXXX nothing changed even after presenting supporting documentation. The account still showing incorrect balance.\n",
      "[{'label': 'NEGATIVE', 'score': 0.9974887371063232}]\n",
      "A COLLECTION HAS BEEN REPORTED TO MY CREDIT REPORT THAT IS NOT MINE.\n",
      "[{'label': 'NEGATIVE', 'score': 0.9996302127838135}]\n",
      "this collection is not mines\n",
      "[{'label': 'POSITIVE', 'score': 0.9877626895904541}]\n",
      "I recently reviewed my credit profile and noticed inaccuracies and false accusations and fraudulent accounts.\n",
      "[{'label': 'NEGATIVE', 'score': 0.995735764503479}]\n",
      "Ive been cosistent paying my bills on time. I'm saying this because I went XXXX XXXX the other day and applied for an account. I got approved but with a {$400.00} deposit. I dint understand until I checked my credit report. I see a couple of items that are not mine. These items need to be deleted from my credit report.\n",
      "[{'label': 'NEGATIVE', 'score': 0.9988757967948914}]\n",
      "I sent in requests to investigate errors on my credit report over 30 days ago, but I have not yet received the response. They were supposed to respond within 30 days, but they have not.\n",
      "[{'label': 'NEGATIVE', 'score': 0.9987154006958008}]\n",
      "I requested for items to be investigated via mail and they have refused to perform the investigation. \n",
      "Please delete the items on the attached letter from my credit report.\n",
      "[{'label': 'NEGATIVE', 'score': 0.9994825124740601}]\n"
     ]
    }
   ],
   "source": [
    "for k in range(10):\n",
    "  print(complaints.values[k])\n",
    "  print(classifier(complaints.values[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGFiOhimXOYq"
   },
   "source": [
    "As you might expect, the complaints dataset has mostly negative values. While this is somewhat of a trivial example, it highlights how in just a few lines of code and no preprocessing we can implement a model on our own data. While this doesn't work for every task, for example the specific classification task that we were working with above, this is a valuable and powerful tool for quick, out-of-the-box models that don't take very long to initialize and tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Model\n",
    "We can use a large language model such as GPT-2 or GPT-Neo with the Hugging Face Transformers library. In this example, we will generate text using the GPT-2 model with the Hugging Face Transformers library. This example will use the code below to produce five different continuations of the provided input text. \n",
    "\n",
    "We will use pipeline function. The pipeline function is a simple way to create a model with its associated preprocessing and postprocessing steps.  It's a high-level function provided by the Hugging Face Transformers library that makes it easy to load a pre-trained model with its associated tokenizer and use it for various tasks like text generation, sentiment analysis, and more. Using the pipeline function is not strictly necessary, but it does simplify the code by abstracting away several underlying details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "# Here, 'text-generation' tells the pipeline to create a text generation model, and model='gpt2' \n",
    "# specifies that you want to use the GPT-2 model for this task.\n",
    "generator = pipeline('text-generation', model='gpt2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(100)\n",
    "input_text = \"I study sociology at University of California\"\n",
    "# specifying the maximum length of the output, and the number of different sequences you want\n",
    "output = generator(input_text, max_length=50, num_return_sequences=5) \n",
    "for result in output:\n",
    "    print(result['generated_text'])\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "We can experiment changing different hyper parameters in the model above. Experiment by specifying different values for *temperature* parameter in generator function.\n",
    "<br>\n",
    "Temperature regulates the unpredictability of a language model's output. With higher temperature settings, outputs become more creative and less predictable as it amplifies the likelihood of less probable tokens while reducing that for more probable ones. A temperature value that is higher, such as 1.7, encourages the generation of text that is more varied and imaginative. Conversely, a lower temperature value, like 0.7, steers the output towards more concentrated and predictable text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generator(input_text, max_length=50, num_return_sequences=5, .. = ..) \n",
    "for result in output:\n",
    "    print(result['generated_text'])\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used GPT2 to generate text above because it is free and runs relatively faster to be able to demonstrate in a classroom setting.  Sometimes, using a larger and more powerful model such as GPT-3 instead of GPT-2 can improve the quality of the generated text. Larger models often capture nuances and subtleties better. However, larger models often take longer time to run and some of the newest models are only available through a paid API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GP_oJhO0WcJV"
   },
   "source": [
    "## Challenge \n",
    "\n",
    "Let's practice with another task from [huggingface](https://huggingface.co/docs/transformers/task_summary). \n",
    "\n",
    "Let's say we want to check our data for grammatical correctness. We will use the CoLA model (\"textattack/distilbert-base-uncased-CoLA\") in the Text Classification pipeline ('text-classification') What is the grammatical correctness of each of the first 15 entries in the cfpb dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cnh_0zQjfEf"
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_pNEaqhpKHq"
   },
   "source": [
    "There are thousands of models on huggingface that can be used for a variety of language tasks. This can be a great way to use the models already available to increase our modeling power. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yLbtugXYBqV"
   },
   "source": [
    "# Next Steps\n",
    "\n",
    "This lab has introduced Colab as a way to use GPUs to speed up processing power and explored further applications of deep learning to natural language processing. \n",
    "\n",
    "In practice, using deep learning for computational social science requires building on the foundational concepts covered in this notebook to implement models with more complicated data and architecture. However, there are many strategies can help you navigate the model ecosystem, some of which we will discuss here: \n",
    "\n",
    "1. Documentation (and other resources like tutorials) is a goldmine of information for implementing particular algorithms and completing specific tasks. This is one reason why reading and translating code written by others is a key skill. \n",
    "\n",
    "2. Debugging and interpreting error messages, as well as leveraging online resources in order to resolve them, is another key concept. Resources like documentation and Stack Overflow help solve common errors and get code working faster. \n",
    "\n",
    "3. Computational resources are important for running complex models. Google Colab has access to GPUs, but does have limitations for large and extended jobs. In those cases, options are paid services such as Google Colab Pro or on-campus [resources](https://docs-research-it.berkeley.edu/services/high-performance-computing/overview/). \n",
    "\n",
    "4. Further resources:\n",
    "  - Deep Learning with Python (Francois Chollet)\n",
    "  - [Huggingface course](https://huggingface.co/course/chapter1/1)\n",
    "  - [Tensorflow](https://www.tensorflow.org/tutorials)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNb13i0N/AcK4udp4CXOm2A",
   "collapsed_sections": [
    "L_zANHRDhplZ",
    "pi2ikVCaoNct",
    "F9xyYBVVRZlE"
   ],
   "include_colab_link": true,
   "name": "Deep Learning and Tensorflow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "19f163d889894c0f83fd41f349dc9ada": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff8cf5b835444cdca63fba8c0d6acc49",
      "placeholder": "",
      "style": "IPY_MODEL_ebf98e51ca7144f4a7b34949464afb64",
      "value": "Downloading: 100%"
     }
    },
    "1c77193b7ef34de1a258ea3865ef8d00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e676cd9ddba471bbe49250834a7345a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2121c86cdd3d45ba8a8441e9409dd478": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e82e3db592ee48299f2c2da59f7508cd",
      "placeholder": "",
      "style": "IPY_MODEL_1e676cd9ddba471bbe49250834a7345a",
      "value": " 226k/226k [00:00&lt;00:00, 1.16MB/s]"
     }
    },
    "23e32ddfec514ae5a5f3fa0443adf6ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "26ff32c8075c4bfbbd599f9684a7cdea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ca131b4aace4bb3acbf17ed6be3ac77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "31b7cb0179b44275923e1ab8709c7383": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36fb982530874258b232df0fe36033f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4bacf0649f254a0d989a162713e8eb52",
      "max": 629,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a8f010ab78494b409ca4b4328529964b",
      "value": 629
     }
    },
    "38589bd193514f709dd5ce7d2e5bedcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "39b737c5a9a147c7b7edf620875926b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b675ac9211d464a8324603501ea571d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4bacf0649f254a0d989a162713e8eb52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "514e37a2cc81490b82ef4e2873927db6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_79fa7eca06fc4bc795eeda6f0c8b71ac",
       "IPY_MODEL_74533b2febb645218408210aa9c4c12a",
       "IPY_MODEL_2121c86cdd3d45ba8a8441e9409dd478"
      ],
      "layout": "IPY_MODEL_faed30b0652748e0950d163036860c05"
     }
    },
    "54b4b9c017734ed48eb5e58ed9a01ee0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "55b479d2a81c4a15b2f582395d55c986": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58a368b25b4b406ea7b6bb91c329c6c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a501a28500024c19892528550569f133",
      "placeholder": "",
      "style": "IPY_MODEL_38589bd193514f709dd5ce7d2e5bedcd",
      "value": " 629/629 [00:00&lt;00:00, 5.07kB/s]"
     }
    },
    "5caaf9bc55d340a3b89976d4aeacb332": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6832ca1251124d82a08b8877ed5734ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c05dce23107d4037a27dbd341edea56c",
       "IPY_MODEL_dae54fa11bfb4ada8c6b403642022574",
       "IPY_MODEL_c0913adaad46457fb9c75a983429e8db"
      ],
      "layout": "IPY_MODEL_c09a653d569648b3884a975eafc2daf9"
     }
    },
    "707caeef65034f9aae75b4299122d3a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f5505a6ec444f93b269c94cfd2043a7",
      "placeholder": "",
      "style": "IPY_MODEL_26ff32c8075c4bfbbd599f9684a7cdea",
      "value": "Downloading: 100%"
     }
    },
    "74533b2febb645218408210aa9c4c12a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe4e711d433740d0a97193210f171286",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2ca131b4aace4bb3acbf17ed6be3ac77",
      "value": 231508
     }
    },
    "7994fbb978ff455a8d0de80b794af494": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_707caeef65034f9aae75b4299122d3a4",
       "IPY_MODEL_df3a99de4f434c3285b45a36c76d3490",
       "IPY_MODEL_7f87188f6c0e47ca8e1b50b0add4f040"
      ],
      "layout": "IPY_MODEL_1c77193b7ef34de1a258ea3865ef8d00"
     }
    },
    "79fa7eca06fc4bc795eeda6f0c8b71ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c485b3fc092b45d2a0fce01bc6b2455c",
      "placeholder": "",
      "style": "IPY_MODEL_23e32ddfec514ae5a5f3fa0443adf6ab",
      "value": "Downloading: 100%"
     }
    },
    "7f87188f6c0e47ca8e1b50b0add4f040": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31b7cb0179b44275923e1ab8709c7383",
      "placeholder": "",
      "style": "IPY_MODEL_f47b513d4d204815916385a3f920b97c",
      "value": " 255M/255M [00:24&lt;00:00, 14.3MB/s]"
     }
    },
    "86034a47f8514079912c35155d146369": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_19f163d889894c0f83fd41f349dc9ada",
       "IPY_MODEL_36fb982530874258b232df0fe36033f4",
       "IPY_MODEL_58a368b25b4b406ea7b6bb91c329c6c6"
      ],
      "layout": "IPY_MODEL_5caaf9bc55d340a3b89976d4aeacb332"
     }
    },
    "8f5505a6ec444f93b269c94cfd2043a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a501a28500024c19892528550569f133": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a70a77484d564944bc8f3ff4f1a8d95b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a8f010ab78494b409ca4b4328529964b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c05dce23107d4037a27dbd341edea56c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b675ac9211d464a8324603501ea571d",
      "placeholder": "",
      "style": "IPY_MODEL_c1f08dbd4ab7467e9b41ff463d8af522",
      "value": "Downloading: 100%"
     }
    },
    "c0913adaad46457fb9c75a983429e8db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55b479d2a81c4a15b2f582395d55c986",
      "placeholder": "",
      "style": "IPY_MODEL_c746a04747a842718b63923829c6c159",
      "value": " 48.0/48.0 [00:00&lt;00:00, 1.02kB/s]"
     }
    },
    "c09a653d569648b3884a975eafc2daf9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1f08dbd4ab7467e9b41ff463d8af522": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c40ffebf7195409e9408ee6adfada1e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c485b3fc092b45d2a0fce01bc6b2455c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c746a04747a842718b63923829c6c159": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dae54fa11bfb4ada8c6b403642022574": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39b737c5a9a147c7b7edf620875926b3",
      "max": 48,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a70a77484d564944bc8f3ff4f1a8d95b",
      "value": 48
     }
    },
    "df3a99de4f434c3285b45a36c76d3490": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54b4b9c017734ed48eb5e58ed9a01ee0",
      "max": 267844284,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c40ffebf7195409e9408ee6adfada1e5",
      "value": 267844284
     }
    },
    "e82e3db592ee48299f2c2da59f7508cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ebf98e51ca7144f4a7b34949464afb64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f47b513d4d204815916385a3f920b97c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "faed30b0652748e0950d163036860c05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe4e711d433740d0a97193210f171286": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff8cf5b835444cdca63fba8c0d6acc49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
