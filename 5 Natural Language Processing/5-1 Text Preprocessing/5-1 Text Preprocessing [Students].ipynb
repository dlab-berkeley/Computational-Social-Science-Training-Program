{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Computational Social Science]\n",
    "## 5-1 Text Preprocessing and Featurization - Student Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab will cover the basics of text preprocessing and featurization, and introduce text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Environment\n",
    "Remember to always activate your virtual environment first before you install packages or run a notebook! This helps to prevent conflicts between dependencies across different projects and ensures that you are using the correct versions of packages. You must have created anaconda virtual enviornment in the `Anaconda Installation` lab. If you have not or want to create a new virtual environment, follow the instruction in the `Anaconda Installation` lab. \n",
    "\n",
    "<br>\n",
    "\n",
    "If you have already created a virtual enviornment, you can run the following command to activate it: \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda activate <virtual_env_name>`\n",
    "\n",
    "<br>\n",
    "\n",
    "For example, if your virtual environment was named as CSS, run the following command. \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda activate CSS`\n",
    "\n",
    "<br>\n",
    "\n",
    "To deactivate your virtual environment after you are done working with the lab, run the following command. \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda deactivate`\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing your environment\n",
    "\n",
    "You will need to uncomment the following lines download `SpaCy` if you do not already have it on your local machine. `NLTK` (Natural Language Toolkit) is an older, less efficient alternative. `SpaCy` is a simplier and optimized library compared to `NLTK`, which is more comprhensive but is a bit more verbose and has a slightly higher learning curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "# ----------\n",
    "import spacy \n",
    "import en_core_web_sm\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "# settings\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../../images/cfpb_logo.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next few labs, we will use the Consumer Financial Protection Bureau's [Consumer Complaint Database](https://www.consumerfinance.gov/data-research/consumer-complaints/). The database is rich with information about individual consumer complaints about credit card fraud, debt collections, and other consumer issues. This dataset is convenient for text analysis because the consumer complaints are real text generated by real people. This means that the data have all the idiosyncrasies that come with those data-generating processes. It also contains multiple different categories that we can predict, like type of product the complaint is about and whether the complaint was resolved quickly. \n",
    "\n",
    "The basic process is that if someone has a dispute related to consumer finance (mortgages, student loans, credit cards, etc.), they can file a dispute with the CFPB, which then contacts the company named in the dispute to get some resolution of the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# ----------\n",
    "\n",
    "# load the dataframe\n",
    "cfpb = pd.read_csv(\"../../data/CFPB 2020 Complaints.csv\")\n",
    "\n",
    "# drop missing on \"Consumer complaint narrative\" feature and reset the index bc we've dropped\n",
    "cfpb = cfpb.dropna(subset = ['Consumer complaint narrative']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check \n",
    "# ----------\n",
    "cfpb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view examples just from \"Consumer complaint\" column \n",
    "# ----------\n",
    "cfpb['Consumer complaint narrative'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the **process of splitting text into words and sentences.** These chunks (words, sentences, etc.) are called **tokens**. One approach might be to try to do this use string methods like [str.split](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html). The problem with this is that using a separator like a \",\" or \".\" or \" \" may not work for some common situations. So instead, we'll use the [spaCy](https://spacy.io/) library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why tokenize?\n",
    "\n",
    "Electronic text is a linear sequence of symbols. Before any processing can be done, text needs to be segmented into linguistic units, and this process is called tokenization.\n",
    "\n",
    "We usually look at grammar and meaning at the level of words, related to each other within sentences, within each document. So if we're starting with raw text, we first need to split the text into sentences, and those sentences into words -- which we call \"tokens\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to tokenize\n",
    "\n",
    "#### Using String Methods\n",
    "\n",
    "##### Split Into Sentences\n",
    "\n",
    "You might imagine that the easiest way to identify sentences is to split the document at every period `\".\"`, and to split the sentences using white space to get the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the split function to create tokens\n",
    "text = cfpb['Consumer complaint narrative'][0] # what is this line doing?\n",
    "sentences = text.split(...)                    # split text by a decimal point\n",
    "for s in sentences[:5]:                        # specify just the first 5 sentences\n",
    "    print(s + '\\n')                            # What does the \"\\\" do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be ok, but what if someone said something like \"U.C. Berkeley charged me $50.11 by mistake.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustration of how this function breaks down\n",
    "# ----------\n",
    "bad_text = \"U.C. Berkeley charged me $50.11 by mistake.\"\n",
    "bad_sentences = bad_text.split(\".\")\n",
    "for s in bad_sentences[:5]:\n",
    "    print(s + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look too good! The one sentence was split into 4 separate sentences because `\".\"`'s are used for things other than ending a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split Into Tokens\n",
    "From here, we might split each sentence into tokens by splitting on white space in between words. Try filling in the code below to take the first sentence and split on white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a different parameter in the split function to create tokens using a \" \" (space)\n",
    "# ----------\n",
    "sentence = ...                          # take the first sentence from our \"sentences\" object\n",
    "print(\"Sentence to split: \", sentence)  # visualize the sentence we we splitting\n",
    "tokens = sentence.split(...)            # split by \" \" a space this time\n",
    "tokens                                  # print the token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHALLENGE 1:** What was the problem with splitting on the white space? Are there any tokens that look a little strange?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCy\n",
    "\n",
    "In contrast to simple string methods, `spaCy` uses pre-trained language models to incorporate context. In this case, we'll load the [en_core_web_sm](https://spacy.io/models/en), which is one of `spaCy`'s English language models. For instance, the end of a sentence (\".\") should mark a new token, but the string \"U.K.\" should not be separated at the `\".\"`'s. According to [spaCy's documentation](https://spacy.io/usage/spacy-101#annotations-token) it achieves this by taking the following steps:\n",
    "\n",
    "First, the raw text is split on whitespace characters, similar to `text.split(' ')`. Then, the tokenizer processes the text from left to right. On each substring, it performs two checks:\n",
    "\n",
    "1. Does the substring match a tokenizer exception rule? For example, “don’t” does not contain whitespace, but should be split into two tokens, “do” and “n’t”, while “U.K.” should always remain one token.\n",
    "2. Can a prefix, suffix or infix be split off? For example, punctuation like commas, periods, hyphens or quotes.\n",
    "\n",
    "If there’s a match, the rule is applied and the tokenizer continues its loop, starting with the newly split substrings. This way, `spaCy` can split complex, nested tokens like combinations of abbreviations and multiple punctuation marks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try applying these methods to our CFPB data. The steps to do this are:\n",
    "\n",
    "1. Load the language model.\n",
    "2. Apply it to a piece of text and save it in an `spaCy` \"doc\" object.\n",
    "3. Extract each token from the doc object to a list.\n",
    "4. Display the tokens\n",
    "\n",
    "Check the documentation for help filling in these steps!\n",
    "\n",
    "Note this next step might take a few moments to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a different parameter in the split function to create tokens using a \" \" (space)\n",
    "# ----------\n",
    "nlp = en_core_web_sm.load()                 # load the small language model en_core_web_sm\n",
    "\n",
    "doc = nlp(...)                              # apply the nlp language model to process the \"text\" object from above\n",
    "spacy_words = [token.text for token in doc] # create a list of words from doc object\n",
    "display(f\"Tokenized words: {spacy_words}\")  # fancier way to displaying an object using the f-string formatting\n",
    "#print(spacy_words)                         # another way that you might be more familier with\n",
    "\n",
    "# note that \"token.text\" is an attribute of \"token\" not the object text we are cleaning\n",
    "# see--https://spacy.io/api/token--for more details\n",
    "\n",
    "# general syntax for list comprehension: [expression for item in iterable]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stop Words and Punctuation\n",
    "\n",
    "We now have some tokens with just a few lines of code! There are a few additional steps that we might want to take. For example, we may want to remove punctuation and stop words. Punctuation oftentimes does not add substantive information to a piece of text, and stop words are common words that appear very frequently across texts. Removing this kind of information can help with downstream classification tasks by allowing an algorithm to focus on words that distinguish documents, rather than ones that appear frequently across them. \n",
    "\n",
    "Note that we might all have different opinions on the precise list of stop words, but in general this library should cover the basics. \n",
    "\n",
    "First, let's take a look at stop words. We can start by importing a collection of stop words from spaCy by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load STOP_WORDS module from spaCy library\n",
    "# ----------\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at some common stop words from this collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list stop words 1-11 from the library\n",
    "# ----------\n",
    "list(STOP_WORDS)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the list of words you might sight will differ because `STOP_WORDS` is a set and not a list. A [set](https://www.w3schools.com/python/python_sets.asp) is an **unordered,** **unchangable,** and **unindexed** data structure in python.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load punctuation module from string library\n",
    "# ----------\n",
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing punctuation and stop words is not a hard and fast rule - there may be situations where you want to keep them. In most applications, they add noise to downstream tasks, but always be mindful of your particular application when making decisions. Now that we have some tokenization tools, let's put them all together in a function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHALLENGE 2:** Write a function that takes a piece of text as an argument, and returns a list of tokens without punctuation or stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load punctuation module from string library\n",
    "# ----------\n",
    "\n",
    "# initalize an empty list\n",
    "tokens_reduced = []\n",
    "\n",
    "# create a function to remove punctuation and stop words \n",
    "def rem_punc_stop(text):\n",
    "    \n",
    "    # set objects\n",
    "    stop_words = ...      # set STOP_WORDS to a new object variable\n",
    "    punc = set(...)       # convert punctuation to a set\n",
    "    \n",
    "    # essentially remove the punctuation - important to remove punctuation first to correctly capture stop words\n",
    "    punc_free = \"\".join([... for ... in ... if ... not in ...]) # join new list of characters (ch) in text w/ condition\n",
    "                                                                # if ch is not in punctuation \n",
    "                                                                # \"\".join() creates a string from the list comprehension\n",
    "\n",
    "    # apply nlp to punctuation-free object\n",
    "    doc = nlp(...)\n",
    "    \n",
    "    # extract words from processed text \n",
    "    spacy_words = [...]\n",
    "    \n",
    "    # filter out words \n",
    "    no_punc = [...]\n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function\n",
    "# ----------\n",
    "\n",
    "# apply our new function to our text object from above\n",
    "tokens_reduced = rem_punc_stop(...)\n",
    "\n",
    "# view the first 5 tokens\n",
    "tokens_reduced[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy also contains a number of methods for things like entity recognition. For instance, we could run the following code to check various entities. Notice that this process isn't perfect, spaCy still thinks \"XX/XX/XXXX\" is an organization or product even though we know this is a redacted date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to text object\n",
    "# ----------\n",
    "# run a loop and print out the \n",
    "for entity in nlp(text).ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How can we modify the above code to apply function to the cleaned tokens_reduced object?\n",
    "# ----------\n",
    "\n",
    "# first, convert tokens_reduced from list to a string\n",
    "tokens_reduced_str = ...\n",
    "\n",
    "# next, apply function\n",
    "for entity in nlp(...).ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another preprocessing step we might take is reducing words down to their lemmas. Lemmatization reduces a word to its root word, while making sure the word still belongs to the language. This is in contrast to stemming, which reduces the word down to its root even if that root is not a valid word. Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a loop to identify the shortest variatiant of each word \n",
    "# ----------\n",
    "for word in nlp(u'compute computer computed computing'):\n",
    "    print(word.text,  word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming these words would all result in the root \"comput\" but lemmatization converted these words to their shortest variant. Again, you may choose to stem or lemmatize depending on your specific application. \n",
    "\n",
    "Now, try it again but with a new set of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it again with a new word to see how it changes \n",
    "# ----------\n",
    "for word in nlp(u'... ... ..'): # choose a new set of words\n",
    "    print(word.text,            # print the word \n",
    "          word.lemma_)          # print the lemma "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHALLENGE 3:** Lemmatize the first consumer complaint narrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now run a loop to identify the lemma for each word in out \"text\" object we've used above \n",
    "# ----------\n",
    "for ... in nlp(...): # specify the text object from above\n",
    "    print(...,       # print the word  \n",
    "          ...)       # print the lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also want to chunk more than one word together. One way to do this might be to group nouns together. \n",
    "\n",
    "**CHALLENGE 4:** Trying using the [`noun_chunks`](https://spacy.io/api/doc#noun_chunks) method to chunk nouns in the first complaint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunking\n",
    "# ----------\n",
    "# apply the nlp to our text object to convert to a spacy object\n",
    "doc = nlp(...)  # apply nlp to text object from above\n",
    "\n",
    "# run a loop to chunk and print the obejct\n",
    "for np in ...:\n",
    "    print(...)  # print the np object calling the text attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have covered some the basics of text preprocessing, we are ready to start getting our data in a format for feeding it into machine learning algorithms. There are many options for converting raw text to features in a supervised machine learning problem. The most basic of these is the \"bag of words\" approach. Bag of words essentially counts the number of times each word appears in a corpus, and these counts become features.\n",
    "\n",
    "To illustrate, first let's import the CounterVectorizer method from sklearn. Once we do that, let's use our tokenizer function that we wrote earlier to initialize the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "# ----------\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize CountVectorizer and specify some parameters\n",
    "# ----------\n",
    "bow_vector = CountVectorizer(tokenizer = rem_punc_stop, # use our function for tokenizing created above\n",
    "                             token_pattern = None,      # set to \"None\" since we have specify our own pattern\n",
    "                             ngram_range=(1,1))         # use default for unigrams - see documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we create a CountVectorizer object, we can then transform a list of texts with the \"fit_transform\" method. This will return a sparse matrix with the counts. We can densify the matrix with the \".todense()\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform just a subset of our data\n",
    "# ----------\n",
    "bow_matrix = bow_vector.fit_transform(cfpb['Consumer complaint narrative'][0:5]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "# ----------\n",
    "bow_matrix.todense() # todense() densifies the sparse matrix (which only stores non-zeros elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the feature names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature names instead and view subset\n",
    "# ----------\n",
    "feature_names = bow_vector.get_feature_names_out()\n",
    "feature_names[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency\n",
    "\n",
    "An extension of bag-of-words is the term frequency-inverse document frequency approach. Whereas bag-of-words counts the number of words in the document, tf-idf takes this quanity and divides it by how frequently the word shows up across the corpus. In doing so, the tf-idf score downweights words that are common in the corpus and thus would not aid with classification.\n",
    "\n",
    "**CHALLENGE 5:** Using the code from the \"Bag of Words\" section as a template, write code to get the tf-idf matrix for the CFPB data. You may want to check the documentation for [TfidfVectorizer()](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to see other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize TfidfVectorizer and specify some parameters\n",
    "# ----------\n",
    "tfidf_vector = TfidfVectorizer(...)   # use our function for tokenizing created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform just a subset of our data\n",
    "# ----------\n",
    "# fit and transform to obtain the tfidf matrix\n",
    "matrix = ... # subset only the first 5 rows\n",
    "\n",
    "# if you want to get the feature names and check the length\n",
    "feature_names = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "# ----------\n",
    "matrix.todense() # todense() densifies the sparse matrix (which only stores non-zeros elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Classification (optional)\n",
    "\n",
    "This section is optional because we won't cover Naive Bayes until later in the semester but it will give you a sense of where we are going and what we can do with cleaned data. \n",
    "\n",
    "So, now that we have featurized our text, we are ready to make a prediction! Does the text of our consumer complaints predict whether or not they get a timely response?\n",
    "\n",
    "**CHALLENGE 6:** Transform the text of the consumer complaint narrative into a tf-idf matrix, and use it to predict the \"Timely response?\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Naive Bayes Classification workflow\n",
    "# --------------------------------------------\n",
    "\n",
    "# subset the data for speed of analysis\n",
    "# ----------\n",
    "cfpb = cfpb[:5000]\n",
    "\n",
    "\n",
    "# features\n",
    "# ----------\n",
    "X = cfpb[...]                          # select just the consumer complaint column\n",
    "tf = ...(tokenizer = ....,             # initialize the TfidfVectorizer() algorithm and use our cleaning algorithim\n",
    "         token_pattern = None)         # set to \"None\" since we have specify our own pattern\n",
    "\n",
    "\n",
    "\n",
    "# fit tokenizer on X dataframe\n",
    "tfidf_matrix =  tf.fit_transform(...) # fit and transform the features\n",
    "\n",
    "\n",
    "# label\n",
    "# ----------\n",
    "y = cfpb['Timely response?']          # subset so outcome object only includes timely response label\n",
    "\n",
    "# xreate train-test split\n",
    "# ----------\n",
    "X_train, X_test, y_train, y_test = train_test_split(...,                # tranformed feature matrix\n",
    "                                                    ...,                # labels\n",
    "                                                    train_size = ...,   # training split\n",
    "                                                    test_size = ...,    # testing split\n",
    "                                                    random_state=100)   # set random state\n",
    "\n",
    "# xreate train-validation split\n",
    "# ----------\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(...,              # feature training data\n",
    "                                                            ...,              # outcome training data\n",
    "                                                            train_size = ..., # training split\n",
    "                                                            test_size = ...,  # testing split\n",
    "                                                            random_state=101) # set random state\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "# ----------\n",
    "from spacy.util import fix_random_seed # important random seed utility from spacy library\n",
    "fix_random_seed(1) # set seed\n",
    "\n",
    "# initialize the Multinomial Naive Bayes algorithim\n",
    "# ----------\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# fit it on the training data\n",
    "# ----------\n",
    "nb_model = nb.fit(..., \n",
    "                  ...)\n",
    "\n",
    "# predict on validation dataset\n",
    "# ----------\n",
    "nb_pred = nb_model.predict(....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy library\n",
    "# ----------\n",
    "import numpy as np\n",
    "\n",
    "# print average accuracy\n",
    "# ----------\n",
    "print(np.mean(nb_pred == ...))\n",
    "\n",
    "# create a confusion matix\n",
    "# ----------\n",
    "nb_cf_matrix = confusion_matrix(..., \n",
    "                                ...\n",
    "                                normalize = \"pred\") # normalize by column \n",
    "nb_cf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the confusion matrix! Use the following code from the \"seaborn\" package to make a heatmap out of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe for visualization\n",
    "# ----------\n",
    "nb_df_cm = pd.DataFrame(nb_cf_matrix, \n",
    "                        range(2),\n",
    "                        range(2))\n",
    "\n",
    "# visualize\n",
    "nb_df_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create confusion matrix using Seaborn code\n",
    "# ----------\n",
    "# add indices and labels\n",
    "nb_df_cm = nb_df_cm.rename(index=str, columns={0: \"no\", 1: \"yes\"})\n",
    "nb_df_cm.index = [\"no\", \"yes\"]\n",
    "\n",
    "# specifyc figure parameters\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.set(font_scale=1.4)#for label size\n",
    "\n",
    "# specify figure using seaborn library\n",
    "sn.heatmap(nb_df_cm, \n",
    "           annot=True,\n",
    "           annot_kws={\"size\": 16}, \n",
    "           fmt='.2f')  # prevent scientific notation\n",
    "\n",
    "# figure labels\n",
    "plt.title(\"Naive Bayes Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
