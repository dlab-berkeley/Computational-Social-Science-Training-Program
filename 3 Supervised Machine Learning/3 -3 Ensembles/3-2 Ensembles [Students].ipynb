{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Computational Social Science]\n",
    "## 3-2 Tree-Based Methods and Ensemble Methods - Student Version\n",
    "\n",
    "In this lab, we will extend on our lab notebook on tree based methods to ensemble learning, which involves combining several machine learning algorithms together to create a better modelË™"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Environment\n",
    "Remember to always activate your virtual environment first before you install packages or run a notebook! This prevents the potential of crashing your root Python/Anaconda installation. You must have created anaconda virtual enviornment in the `Anaconda Installation` lab. If you have not or want to create a new virtual environment, follow the instruction in the `Anaconda Installation` lab. If you have already created a virtual enviornment, you can run the following command to activate it. <br>\n",
    "`conda activate <virtual_env_name>`\n",
    "For example, if your virtual environment was named as legal-studies, run the following command. <br>\n",
    "`conda activate legal-studies`\n",
    "To deactivate your virtual environment after you are done working with the lab, run the following command. <br>\n",
    "`conda deactivate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We're going to use our Census Income dataset dataset again for this lab. Load the dataset in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "#sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of column names, found in \"adult.names\"\n",
    "col_names = ['age', 'workclass', 'fnlwgt',\n",
    "            'education', 'education-num',\n",
    "            'marital-status', 'occupation', \n",
    "             'relationship', 'race', \n",
    "             'sex', 'capital-gain',\n",
    "            'capital-loss', 'hours-per-week',\n",
    "            'native-country', 'income-bracket']\n",
    "\n",
    "# Read table from the data folder\n",
    "census = pd.read_table(\"../../data/adult.data\", sep = ',', names = col_names)\n",
    "census.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we need to preprocess the data to binarize the target and dummify our categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target\n",
    "lb_style = LabelBinarizer()\n",
    "y = census['income-bracket-binary'] = lb_style.fit_transform(census[\"income-bracket\"])\n",
    "\n",
    "# Features\n",
    "X = census.drop(['income-bracket', 'fnlwgt', 'income-bracket-binary'], axis = 1)\n",
    "X = pd.get_dummies(X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model we will look at is the decision tree. Using the [`tree.DecisionTreeClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) method, let's implement a cross-validation approach to predicting income. We will initialize the model with the standard configurations from the Classification lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Decision Tree Classifier\n",
    "dt_classifier = tree.DecisionTreeClassifier(criterion='gini',  # or 'entropy' for information gain\n",
    "                       splitter='best',  # or 'random' for random best split\n",
    "                       max_depth=None,  # how deep tree nodes can go\n",
    "                       min_samples_split=2,  # samples needed to split node\n",
    "                       min_samples_leaf=1,  # samples needed for a leaf\n",
    "                       min_weight_fraction_leaf=0.0,  # weight of samples needed for a node\n",
    "                       max_features=None,  # number of features to look for when splitting\n",
    "                       max_leaf_nodes=None,  # max nodes\n",
    "                       min_impurity_decrease=1e-07, #early stopping\n",
    "                       random_state = 10)  #random seed\n",
    "                       #class_weight = \"balanced\") # balanced class prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_val_score returns the accuracy score by default but you can change this with the \"scoring\" argument\n",
    "scores = cross_val_score(dt_classifier, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the mean score from the results of cross validation\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".82 accuracy, not bad! We can also visualize the decision tree to see how it made its splits. Note we limit the max depth to 4 so that the code runs quickly, but in practice you might want to visualize the entire tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier.fit(X, y)\n",
    "\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(dt_classifier, \n",
    "                   feature_names=X.columns,  \n",
    "                   class_names=[\"<=50k\", \">50k\"],\n",
    "                   filled=True,\n",
    "                  fontsize = 10,\n",
    "                  max_depth = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the .max_depth attribute to check out the depth of our entire tree\n",
    "dt_classifier.tree_.max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remind ourselves how many samples in our negative class\n",
    "np.count_nonzero(y==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the samples after root node\n",
    "X['marital-status_ Married-civ-spouse'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the most informative features\n",
    "importances = pd.DataFrame({'feature':X.columns,'importance':np.round(dt_classifier.feature_importances_,3)})\n",
    "importances = importances.sort_values('importance',ascending=False)\n",
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "Ensemble learning is a machine learning paradigm where multiple learners (also known as base or individual models) are trained to solve the same problem. The main idea behind ensemble learning is that a group of \"weak learners\" can come together to form a \"strong learner\". Each weak learner makes a prediction, and then the ensemble model makes its final prediction based on the votes or the outputs of all the weak learners.\n",
    "\n",
    "Ensemble learning often significantly improves machine learning results by combining several models. This approach allows the production of better predictive performance compared to a single model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Next, we'll take a look at the [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). Random Forest is an extension of the decision tree method. Rather than construct just one tree, a random forest grows many trees, using a subset of features to grow each tree. The trees then make predictions, and the random forest takes a majority vote from the trees to determine the winner. Random forest is known as a [\"bagging\"](https://en.wikipedia.org/wiki/Bootstrap_aggregating) method. Fill in the code below to train a random forest using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a random forest\n",
    "rf_classifier = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some algorithms will expect you to ravel the target\n",
    "scores = cross_val_score(..., ..., ..., ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is difficult to visualize a forest of trees, we *can* still visualize the feature importances. Use the code below to look at the top 10 most important features. What do you notice? Do you think we actually need a large feature space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit the random forest on the whole dataset to get feature importances\n",
    "rf_classifier.fit(X, y.ravel())\n",
    "import seaborn as sns\n",
    "\n",
    "feat_importances = pd.concat([pd.DataFrame(X.columns),pd.DataFrame(np.transpose(rf_classifier.feature_importances_))], axis = 1)\n",
    "feat_importances.columns = [\"Feature\", \"Importance\"]\n",
    "sns.barplot(x = \"Importance\", y = \"Feature\", data = feat_importances.nlargest(10, 'Importance'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that only a handful of features are contributing a lot to the model. We could probably simplify the decisionmaking considerably. Try training a new decision tree with max depth 5 and only use the 10 most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_reduced_classifier = ...\n",
    "\n",
    "important_features = feat_importances.nlargest(10, 'Importance')['Feature']\n",
    "\n",
    "X_reduced = X[X.columns[X.columns.isin(important_features)]]\n",
    "\n",
    "dt_reduced_classifier.fit(..., ...)\n",
    "fig = plt.figure(figsize=(45,40))\n",
    "_ = tree.plot_tree(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks a lot more interpretable than a random forest! How did we do on accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(..., ..., ..., ...)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost .85! Not quite as good as the whole random forest, but better than our original decision tree. Growing a random forest and then simplifying down to a more basic decision tree is the basic procedure recommended by the [select-regress-round](https://arxiv.org/pdf/1702.04690.pdf) framework.\n",
    "\n",
    "**Question**: Why did a simplified decision tree get better accuracy than the first one we ran?\n",
    "\n",
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other approach for ensembling decision trees is called [\"boosting\"](https://en.wikipedia.org/wiki/Boosting_(machine_learning). Whereas random forests grow many decision trees in parallel and take a vote from them, boosting algorithms start with a strong classifier and iterate on it with weak learners. The weak learners are trained on the errors, which makes boosting algorithms well suited for making classifications in difficult cases. Try filling in the code below to train an [`AdaBoostClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_classifier = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install xgboost\n",
    "import ... as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGBoost classifier\n",
    "xgb_classifier = xgb.XGBClassifier(random_state=..)\n",
    "\n",
    "\n",
    "# Define the scoring metrics\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(..),\n",
    "    'recall': make_scorer(..),\n",
    "    'precision': make_scorer(..),\n",
    "    'f1': make_scorer(..)\n",
    "}\n",
    "\n",
    "# Perform cross-validation with 5-fold and return the trained estimators\n",
    "cv_results = cross_validate(xgb_classifier, X, y.ravel(), cv=5, return_estimator=True, scoring=scoring)\n",
    "\n",
    "# Print the results for accuracy, recall, precision, and F1 score\n",
    "for metric in ['test_accuracy', 'test_recall', 'test_precision', 'test_f1']:\n",
    "    scores = cv_results[metric]\n",
    "    print(f\"{metric[5:]}: {scores.mean():.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an array to hold the feature importances\n",
    "importances = np.zeros(X.shape[1])\n",
    "\n",
    "# Average the feature importances over the folds\n",
    "for estimator in cv_results['estimator']:\n",
    "    importances += estimator.feature_importances_\n",
    "\n",
    "importances /= ..  # Divide by the number of folds\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "feature_importance = pd.DataFrame({'feature': X.columns, 'importance': importances})\n",
    "\n",
    "# Sort the features by importance\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "# Take the top 10 features\n",
    "feature_importance = feature_importance.head(..)\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "# horizontal bar plot\n",
    "plt.barh(feature_importance['feature'], feature_importance['importance'], color='b', align='center')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importance')\n",
    "plt.gca().invert_yaxis() # gca stands for \"get current axis\", which allows you to modify the properties of the axes.\n",
    "# is a method that inverts the y-axis, meaning that the values that were at the bottom will now be at the top, and vice versa.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning Beyond Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create ensembles with algorithms beyond decision trees. Scikit's ensemble module contains several different options for training ensemble models. Here, we will focus on the [`VotingClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) method. A voting classifier works in a similar fashion to random forest. However, instead of taking a majority vote of decision trees, it takes a majority vote of various algorithms. The voting can be \"hard\" which means the ensemble uses a majority vote of predicted classes, or \"soft\" meaning the votes are weighted by the probability associated with the prediction. Run the code below to initialize a logistic regression, a random forest, and an adaboost model. Pass all three of these into the VotingClassifier to train an ensemble model, and check out their accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression using liblinear solver\n",
    "log_reg = LogisticRegression(random_state = 10, solver='liblinear')\n",
    "\n",
    "# Random Forest\n",
    "rf_classifier = RandomForestClassifier(criterion='gini',  # or 'entropy' for information gain\n",
    "                       max_depth=None,  # how deep tree nodes can go\n",
    "                       min_samples_split=2,  # samples needed to split node\n",
    "                       min_samples_leaf=1,  # samples needed for a leaf\n",
    "                       min_weight_fraction_leaf=0.0,  # weight of samples needed for a node\n",
    "                       max_features=None,  # number of features to look for when splitting\n",
    "                       max_leaf_nodes=None,  # max nodes\n",
    "                       min_impurity_decrease=1e-07, #early stopping\n",
    "                       random_state = 10) #random seed\n",
    "\n",
    "# AdaBoost\n",
    "ada_classifier = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "voting_classifier = VotingClassifier(\n",
    "                        estimators = [('lr', log_reg),\n",
    "                                     ('rf', rf_classifier),\n",
    "                                     ('ada', ada_classifier)],\n",
    "                        voting = 'hard')\n",
    "\n",
    "# Loop through each model to report Accuracy\n",
    "for clf, label in zip([log_reg, \n",
    "                       rf_classifier, \n",
    "                       ada_classifier, \n",
    "                       voting_classifier], ['Logistic Regression', \n",
    "                                            'Random Forest', \n",
    "                                            'Ada Boost',\n",
    "                                            'Ensemble']):\n",
    "         scores = cross_val_score(clf, X, y.ravel(), scoring='accuracy', cv=5)\n",
    "         print('Accuracy: %0.2f [%s]' % (scores.mean(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did the ensemble do? Next, try to use a soft voting classifier to get the predicted probabilities for each prediction. Try using the `predict_proba()` method to get the predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_classifier = VotingClassifier(...)\n",
    "\n",
    "probas = [c.fit(X, y.ravel()). ...(X)[:,1] for c in (..., ..., ..., ...)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put our predicted probabilities into a dataframe so we can visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_df = pd.DataFrame.from_records(probas).T\n",
    "probs_df.rename(columns = {0: 'logit',\n",
    "                          1: 'rf',\n",
    "                          2: 'ada',\n",
    "                          3: 'ensemble'}, inplace = True)\n",
    "probs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "ax = fig.add_subplot(2, 2, 1)\n",
    "sns.histplot(probs_df, x = \"logit\", ax = ax)\n",
    "ax = fig.add_subplot(2, 2, 2)\n",
    "sns.histplot(probs_df, x = \"rf\", ax = ax)\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "sns.histplot(probs_df, x = \"ada\", ax = ax)\n",
    "ax = fig.add_subplot(2, 2, 4)\n",
    "sns.histplot(probs_df, x = \"ensemble\", ax = ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Do you notice something about the distribution of the predicted probabilities? Can you explain the output of `AdaboostClassifier`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Authored by Aniket Kesari. Minor edits by Tom van Nuenen 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
