{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Computational Social Science] \n",
    "## 3-1 Classification - Student Version\n",
    "\n",
    "In this lab we will cover **Classification** methods. Some of this might look familiar from your previous statistics courses where you fit models on binary or categorical outcomes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Environment\n",
    "Remember to always activate your virtual environment first before you install packages or run a notebook! This helps to prevent conflicts between dependencies across different projects and ensures that you are using the correct versions of packages. You must have created anaconda virtual enviornment in the `Anaconda Installation` lab. If you have not or want to create a new virtual environment, follow the instruction in the `Anaconda Installation` lab. \n",
    "\n",
    "<br>\n",
    "\n",
    "If you have already created a virtual enviornment, you can run the following command to activate it: \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda activate <virtual_env_name>`\n",
    "\n",
    "<br>\n",
    "\n",
    "For example, if your virtual environment was named as CSS, run the following command. \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda activate CSS`\n",
    "\n",
    "<br>\n",
    "\n",
    "To deactivate your virtual environment after you are done working with the lab, run the following command. \n",
    "\n",
    "<br>\n",
    "\n",
    "`conda deactivate`\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We're going to use our [Census Income dataset](https://archive.ics.uci.edu/ml/datasets/Census+Income) dataset again for this lab. Load the dataset in, and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# settings\n",
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of column names, found in \"adult.names\"\n",
    "col_names = ['age', \n",
    "             'workclass', \n",
    "             'fnlwgt',\n",
    "             'education', \n",
    "             'education-num',\n",
    "             'marital-status', \n",
    "             'occupation', \n",
    "             'relationship', \n",
    "             'race', \n",
    "             'sex', \n",
    "             'capital-gain',\n",
    "             'capital-loss', \n",
    "             'hours-per-week',\n",
    "             'native-country', \n",
    "             'income-bracket']\n",
    "\n",
    "# Read table from the data folder\n",
    "census = pd.read_table(\"../../data/adult.data\", sep = ',', names = col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the dataset\n",
    "census..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at unique values of income-bracket\n",
    "census..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Recall that before we try to train machine learning models on a dataset like this, we need to preprocess it. Let's preprocess the data to get it ready for training machine learning algorithms. Then, create a dataframe, **X**, that contains all of the features, and an array, **y**, that contains the target. *Note if you create a Pandas series with y it will throw an error later in the script.*\n",
    "\n",
    "*Hint: Use `LabelBinarizer()` to transform your **y** variable, and `get_dummies()` to create dummy variables for your **X**'s.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target\n",
    "# ----------\n",
    "lb = ...                                # LabelBinarizer creates 1 column from binary labeled column \n",
    "y = census['income-bracket-binary'] =   # transform and fit in same step   \n",
    "\n",
    "# Features\n",
    "# ----------\n",
    "X_prep  = ...     # drop income-bracket varaible\n",
    "X = ...           # get dummies\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Balance\n",
    "\n",
    "Before we start modeling, let's look at the distribution of the target variable. Visualize the distribution of the target variable (\"income-bracket\").\n",
    "\n",
    "**QUESTION:** What do you notice? What do you think this pattern suggests about how easy or difficult it would be for a machine learning model to make the correct classifications?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution plot of the outcome variable\n",
    "sns.displot(y)   # notice the default is a histogram\n",
    "plt.title(\"Distribution of Target Variable (Income Bracket)\")\n",
    "plt.xlabel('Income Bracket')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting\n",
    "\n",
    "Split the data into train, validation, and test sets. Be sure to stratify the splits by the outcome so that we have as representative of a breakdown of outcome in our training/test/validations datasets as we can. To do this, you will need to specify the appropriate option in the `train_test_split()` fuction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(10)\n",
    "\n",
    "# training and test split \n",
    "X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "# training and validation split \n",
    "X_train, X_validate, y_train, y_validate = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). This example should look familiar from the Introduction to Machine Learning lab. Make a logistic regression model, fit it to the training data, and predict on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model\n",
    "logit_reg = ...\n",
    "\n",
    "# fit the model\n",
    "logit_model = ...\n",
    "\n",
    "# predict on the validation data\n",
    "y_pred = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a dataframe with the features and the logit coefficients (**Note**: For the logit coefficients we can use `np.transpose`, or extract the coefficients from the 1d array). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the coefficents and create a dataframe for plotting\n",
    "logit_data = pd.concat([pd.DataFrame(X.columns),pd.DataFrame(np.transpose(logit_model.coef_))], axis = 1)\n",
    "logit_data.columns = ['Feature', 'Coefficient']\n",
    "logit_data['abs_coef'] = abs(logit_data['Coefficient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot the 10 coefficients with the largest absolute value based on the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \n",
    "sns.barplot(...)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a confusion matrix to visualize how well you did with your predictions. \n",
    "\n",
    "**QUESTION:** What do you notice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix\n",
    "cf_matrix = confusion_matrix(...,\n",
    "                             ..., \n",
    "                             normalize = \"true\")\n",
    "\n",
    "# convert to a dataframe\n",
    "df_cm = pd.DataFrame(cf_matrix, range(2), range(2))\n",
    "\n",
    "# set figure specifications\n",
    "df_cm = df_cm.rename(index=str, columns={0: \"<=50k\", 1: \">50k\"})\n",
    "df_cm.index = [\"<=50k\", \">50k\"]\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, \n",
    "           annot=True,\n",
    "           annot_kws={\"size\": 16},\n",
    "           fmt='g')\n",
    "\n",
    "# plot \n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with sklearn's regression methods, we can also use [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to search for optimal hyperparameters. \n",
    "\n",
    "Let's apply the grid search to a logistic regresssion model to find the best hyperparameter values. **Note**: You might notice that the grid search takes a **very** long time to complete depending on the model and hyperparameters chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "# ----------\n",
    "\n",
    "# import libraries\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# set parameters\n",
    "param_grid = ...\n",
    "\n",
    "# execute the grid search and fit to training data\n",
    "logit_grid = GridSearchCV(..., \n",
    "                          param_grid, \n",
    "                          cv=3)\n",
    "logit_grid.fit(..., \n",
    "               ...)\n",
    "\n",
    "# choose best performing model\n",
    "best_index = np.argmax(logit_grid.cv_results_[\"mean_test_score\"])\n",
    "best_logit_pred = logit_grid.best_estimator_.predict(X_validate)\n",
    "\n",
    "# print results\n",
    "print(logit_grid.cv_results_[\"params\"][best_index])\n",
    "print('Validation Accuracy', accuracy_score(best_logit_pred, y_validate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, accuracy isn't the only metric that we might care about. **Accuracy** is an expression of ratio of correct observations relative to incorrect observations. This calculation alone does not tell us much about whether we did a good job predicting all of the other categories that we might be concerned about.\n",
    "\n",
    "Consider our census dataset. We saw earlier that the target data is not equally distributed - there were far more people with \"<=50k\" income. As we saw in our confusion matrices, our algorithms tended to predict observations belonging to the \"<=50k\" category remarkably well, but tended to do much worse with the \">50k\" category. \n",
    "\n",
    "**QUESTION:** Why do you think this might be the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a few metrics that will help us move beyond accuracy as our only measure:\n",
    "\n",
    "$$\n",
    "True \\space Positives = \\sum({Predicted \\space Positives = Observed \\space Positives})\n",
    "$$\n",
    "\n",
    "$$\n",
    "False \\space Positives = \\sum({Predicted \\space Positives \\space != Observed \\space Positives})\n",
    "$$\n",
    "\n",
    "$$\n",
    "True \\space Negatives = \\sum({Predicted \\space Negatives = Observed \\space Negatives})\n",
    "$$\n",
    "\n",
    "$$\n",
    "False \\space Negatives = \\sum({Predicted \\space Negatives \\space != Observed \\space Negatives})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we were primarily interested in detecting whether someone is \">50k\". We'll call this the \"positive\" class. A \"predicted\" observation is the value the model predicted, while the \"observed\" observation is the value in the ground-truth labels. So a **\"true positive\"** in this case would be instances when the model predicted someone to be in the \">50k\" category AND they were in the \">50k\" category in reality. Similarly, a **false positive** would be instances where the model predicted someone was in the \">50k\" category when they were actually in the \"<=50k\" category in reality. \n",
    "\n",
    "Use your best model from hyperparameter to predict on the validation set and see how you did on each of these metrics. Since, the confusion matrix is actually a great way to visualize all of these, let's use that. \n",
    "\n",
    "**QUESTION:** What does each quadrant of the matrix correspond to in terms of these metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify confusion matrix\n",
    "cf_matrix = confusion_matrix(..., \n",
    "                             ..., \n",
    "                             normalize = \"true\")\n",
    "\n",
    "# convert to dataframe\n",
    "df_cm = pd.DataFrame(cf_matrix, \n",
    "                     range(2),\n",
    "                     range(2))\n",
    "\n",
    "# label dataframe \n",
    "df_cm = df_cm.rename(index=str, columns={0: \"<=50k\", 1: \">50k\"})\n",
    "df_cm.index = [\"<=50k\", \">50k\"]\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize = (10,7))\n",
    "sns.set(font_scale=1.4)#for label size\n",
    "sns.heatmap(df_cm, \n",
    "           annot=True,\n",
    "           annot_kws={\"size\": 16},\n",
    "           fmt='g')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These metrics matter in the social sciences because we usually are not given balanced datasets, and we are oftentimes concerned with predicting rare events. Predicting rare events like fraud, credit defaults, and mortality is difficult. Optimizing on accuracy alone can be misleading if the algorithm just guesses the majority class every time without ever predicting the outcome of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the metrics we defined: **True Positives (TP)**, **False Positives (FP)**, **True Negatives (TN)**, and **False Negatives (FN)**. Accuracy can be expressed as:\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "**QUESTION:** In plain language, what does this formula represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to calculate the accuracy of your logistic regression. Calculate the number of true positives, false positives, true negatives, and false negatives and then calculate and print the accuracy.\n",
    "\n",
    "*Tip: initialize the variables with 0 and use the `i` iterator in the for-loop when indexing your `y_validate` and `y_pred` variables.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set objects to 0 to start\n",
    "TP = ...\n",
    "FP = ...\n",
    "TN = ...\n",
    "FN = ...\n",
    "\n",
    "# loop over each observation to assign to TP, FP, TN, and FN\n",
    "for i in range(len(y_pred)): \n",
    "    # True Positives. Hint, what two vectors need to equal each other and 1?\n",
    "    if ... == ... == 1:\n",
    "        TP += 1               \n",
    "    # False Positive. Hint, what vector needs to equal 1 while the other equals 0?\n",
    "    if ... == 1 and ... != ...:\n",
    "        FP += 1\n",
    "    # True Negative\n",
    "    if ... == ... ==0:\n",
    "        TN += 1\n",
    "    # False Negative\n",
    "    if ... == 0 and y_pred[i]!= ...:\n",
    "        FN += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy based on the formula defined above\n",
    "accuracy = ...\n",
    "print(\"Accuracy is\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is a measure of how well calibrated predictions are. The formula for precision is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**: In plain language, what does this formula tell us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and print the precision for the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate precision based on the formula defined above\n",
    "precision = ...\n",
    "print(\"Precision is\", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall is defined as:\n",
    "\n",
    "$$\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**: In plain language, what does the formula tell us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the recall for our logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate recall based on the formula defined above\n",
    "recall = ...\n",
    "print(\"Recall is\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**: How did we do on precision and recall? Could you optimize for one or the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision-recall tradeoff can be managed in a few different ways. One popular metric is the F1 score. It is defined as:\n",
    "\n",
    "$$\n",
    "F1 = 2 * \\frac{precision * recall}{precision + recall}\n",
    "$$\n",
    "\n",
    "Calculate and print the f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the F1 Score based on the formula defined above\n",
    "f1 = ...\n",
    "print(\"F1 Score is\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**: How does F1 trade off between precision and recall? What are the advantages and disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC-ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Area Under the Curve - Receiver Operating Characteristic (AUC-ROC)](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) is a popular method for seeing how well an algorithm does at separating between two classes. It is calculated by plotting the True Positive Rate against the False Positive Rate. Let's define these quantities:\n",
    "\n",
    "$$\n",
    "True \\space Positive \\space Rate(TPR) = Sensitivity = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Hm, this formula looks familiar. In fact, it is exactly the same as Recall! Meanwhile, the False Positive Rate is:\n",
    "\n",
    "$$\n",
    "False \\space Positive \\space Rate (FPR) = 1 - Specificity = \\frac{FP}{TN + FP}\n",
    "$$\n",
    "\n",
    "**QUESTION**: Why does plotting TPR against FPR express separability between class labels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the following code to plot the AUC-ROC for the logistic regression and a \"no skill\" model. Make sure to look up documentation as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve and AUC\n",
    "\n",
    "# split into train/test sets\n",
    "# generate a no skill prediction (majority class)\n",
    "ns_probs = [0 for _ in range(len(y_validate))]\n",
    "\n",
    "# predict probabilities for logistic regression - enter your validation X!\n",
    "lr_probs = logit_model.predict_proba(...)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = ...\n",
    "\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(...)\n",
    "lr_auc = roc_auc_score(...)\n",
    "\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(...)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(...)\n",
    "\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**: How did the logistic regression do on the AUC-ROC metric? Compared to the \"no skill\" decision rule, was it a meaningful improvement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over and Under Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that imbalanced data can cause all sorts of problems and give us misleading results, especially if we only focus on accuracy. How can we correct for these problems? One simple method is to **resample** the data. For example, you might **oversample** the minority class or **undersample** the majority class. Let's use the [**imblearn**](https://imbalanced-learn.readthedocs.io/en/stable/api.html) to try this out. \n",
    "\n",
    "First, you might need to run the cell below to install the library. Anytime you use \"!\" in a Jupyter notebook, this will actually run a bash command. If it's already there, you'll get a message that the requirement is already satisifed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install imbalance-learn \n",
    "# MAKE SURE TO INSTALL OLDER VERSION OR ELSE IT WON'T RUN, BECAUSE IT'S NOT COMPATIBLE WITH CURRENT SCIKIT-LEARN\n",
    "!pip install imbalanced-learn==0.09.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import the RandomOverSampler and RandomUnderSampler methods. \n",
    "\n",
    "**QUESTION**: Why would we resample the training set, instead of the dataset or the validation/test sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call libraries\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next use either the `RandomOverSampler` or `RandomUnderSampler` to resample the training set.\n",
    "*Hint: we need to call the [`.fit_resample`](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html) method on our X_train and y_train variables.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intitate instance of sampler and specify strategy\n",
    "random_over_sampler = RandomOverSampler(sampling_strategy=0.5)\n",
    "random_under_sampler = RandomUnderSampler(sampling_strategy=0.5,\n",
    "                                          replacement = True) # be sure to specify sampling with replace)\n",
    "\n",
    "# fit to training data\n",
    "X_train_new, y_train_new = random_under_sampler..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**: What do you notice about the resampled training targets? What might be some issues with over and undersampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrain the logistic regression model on the newly resampled data. How does AUC-ROC change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model prep \n",
    "# -----------\n",
    "\n",
    "# fit the model\n",
    "logit_model = ...\n",
    "\n",
    "# prediction to validation data\n",
    "y_pred = ...\n",
    "\n",
    "# roc curve and auc\n",
    "# -----------\n",
    "\n",
    "# split into train/test sets\n",
    "# generate a no skill prediction (majority class)\n",
    "ns_probs = [0 for _ in range(len(y_validate))]\n",
    "\n",
    "# predict probabilities for logistic regression\n",
    "lr_probs = logit_model.predict_proba(X_validate)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "\n",
    "# calculate scores\n",
    "ns_auc = roc_auc_score(y_validate, ns_probs)\n",
    "lr_auc = roc_auc_score(y_validate, lr_probs)\n",
    "\n",
    "# summarize scores\n",
    "print('No Skill: ROC AUC=%.3f' % (ns_auc))\n",
    "print('Logistic: ROC AUC=%.3f' % (lr_auc))\n",
    "\n",
    "# calculate roc curves\n",
    "ns_fpr, ns_tpr, _ = roc_curve(y_validate, ns_probs)\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_validate, lr_probs)\n",
    "\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, while sklearn puts together many of the methods we need to train, predict, and visualize the results of our machine learning, there are a lot of substantive choices involved. As you can see, even a slightly imbalanced dataset can cause problems. If you optimize only on accuracy, you might miss relevant aspects of the problem. Be mindful of the various metrics available, and decide which ones best answer the scientific question you have in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Authored by Aniket Kesari. Updated by K. Quinn Fall 2021, by Tom van Nuenen Fall 2022, and Kasey Zapatka Fall 2023."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
